#+TITLE: Ensemble Optimization for Histological Image Classification
#+Author: Eid Alkhaldi
#+Email: eid.alkhaldi@gmail.com
#+LATEX_CLASS: beamer
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{algorithmic}
#+LATEX_HEADER: \usepackage{algpseudocode}

#+EXCLUDE_TAGS: noexport
#+OPTIONS: H:2 toc:nil num:nil
#+BEAMER_THEME: Antibes
#+PROPERTY:  header-args :eval no

* Motivation and Challenges
** Importance of Breast Histological image classification
    * Breast cancer is the most common type of cancer amongst women worldwide
    * More than 10% of women in the U.S. will be diagnosed with breast cancer 
    * Microscopic analysis of biopsy is the most accurate diagnosis method
    * Demands extensive work
** Importance of Breast Histological image classification
    * Requires specialized expertise (pathologists)
    * Time consuming due to scarcity of pathologists
    * Expensive to hire 
    * Pathologists often disagree
** Importance of Breast Histological image classification
 Early diagnosis is important for:
  - Prognosis
  - Reducing healthcare costs
  - Better chance of recovery
** Importance of Breast Histological image classification
 * An automated intelligent approach is needed to compensate for the time delay of analysis caused by the insufficient number of pathologists 
 * The aim of histology image classification methods is to identify abnormalities in the specimens' structures and specify their carcinogenic level
** Challenges 
 - Deep Learning (DL) showed remarkable competence
   + Extract complex features
   + Expands class separability better than conventional ML
   + CNNs are the most reliable DL models
- Limitations:
  + CNNs has a huge number of parameters
  + It demands a high quantity of labeled images
  + Curse of Dimensionality
    * The larger the dataset, the more training time is required
** Challenges 
  + Consequently, end-to-end training of CNNs suffered significantly, due to the scarcity of annotated hematoxylin and eosin (H&E) stained histology images.
  + End-to-end training is data greedy
  + End-to-end training using an insufficient number of classified examples is prone to overfitting regardless of how the weights were initialized
** Challenges 
- Transfer learning is the reusing of a model trained on a large dataset for a different task
- The general layers learns general features
- Training only the task specific layers is very efficient
** Challenges 
- Transfer Learning proved to be a more reliable alternative to  end-to-end training
  + Improves accuracy and robustness
  + Reduces the training time significantly, particularly for small datasets
- Limitations:
  + Could result in overfitting, mainly when training the general layers
  + It lacks the ability to separate task-specific layers from general ones
** Challenges 
- Ensembles are a vertical or cascaded combination of the features or the predictions of accurate models to produce a better performing model
  + Ensembling a number of models proved to be very effective and more robust alternative
  + For ensembles to perform better than its individual model, the models have to be heterogeneous
    * Why?
      - To avoid making the same misclassifications
* Proposed Solutions	
** Proposed Solutions
    * Genetically Optimized Heterogeneous Ensemble for Histolgy Image Classification
    * Adaptive PSO-Based Ensemble Optimization for Histology Image Classification
    * Ensemble Optimization Using Cartesian Genetic Programming
    * Ensemble Optimization Using Colonal Selection Algorithm
* Genetically Optimized Heterogeneous Ensemble for Histolgy Image Classification
** Meta-training 
   The first proposed methods is composed of two phases:
    + Meta-training
    + Ensemble training
   The meta-training is composed of three stages:
      + Preprocessing
      + Transfer Learning
** Preprocessing
   + The aim of preprocessing is to augment the dataset
   + The histological datasets are usually small due to the lack of data
   + Affine Transformation applied
     * random cropping
     * rotation
     * width/height shifting
     * vertical/horizontal flipping
   + Resizeing and cropping to a lower resoluiton
   + Normalization
* Transfer Learning
** Introduction 
   + Transfer Learning is the process of re-utilization of models' weights that were trained on benchmark datasets such as ImageNet
     * More reliable, robust and accurate than end-to-end
     * More time-efficient
   + The weights of a model that was trained end-to-end is used as the initialization for the training of images in a different domain
   + For the new domain, the parameters of the domain-specific layers only are trainable
   + All other weights are frozen
    
** Gaussian Dropout
 \begin{equation} \label{gaus} \sigma _(gaussian)= \sqrt{rate/(1 - rate)}
   \end{equation} 
   Where /rate/ denotes the user control parameter
   and \(\sigma _(gaussian)\) refers the dropout gaussian standard deviation
+ Why?
  +  Blocking several units from firing during the feedforward and the backpropagation steps of training
+ restricts the classifier from learning the irrelevant particularities of an image
 
** Learning Rate Scheduler
     + Aims to minimize the cost function
     + Avoids saddle points in the search spaces which are common in high dimensional problems when LR is too small
     + However, over-increasing the LR will cause fluctuations with high spikes as shown in figure [[fig:LR]]
** Learning Rate Scheduler
#+NAME: fig:LR
#+ATTR_LATEX: :center :width 12cm :height 4cm
#+CAPTION: High spikes of validiation loss and accuracy curves over exponential cyclical learning rate in a single epoch in ResNet50 with freeze layer = 141 and a batch size = 32. The learning rate with the lowest validation loss at lr = 0.05 is chosen as the base lr for the full training.
[[/home/alkhaldieid/Dropbox/finalV/diagrams/first/cyc.png]]
** GA for optimum freeze layer selection
    + The objective of implementing the genetic algorithm is to evolve the solutions to the fittest task-specific layer 
    + GA is an optimization method inspired by simulating the natural selection through crossover and mutation cite:Singh2018
    + The possible layers can't be batch normalizaiton layers or the first 20% of layers
    + Each possible layer is encoded by a binary chromosome
    + After calculating the initial population fitnesses, the fittest layers are chosen to crossover at a predefined crossover probability Pc to produce better offsprings in the next generation.
    + All layers, except BN, are frozen upto the decoded layer
** GA for optimum freeze layer selection
    + cyclical LR is implemented to pick the LR that is most appropriate for the decoded freeze-layer
    + The model is then trained on lower resolution images for 10 epochs
    + The cross-entropy loss is then calculated on the validation dataset
    + The fittest model is then trained thoroughly with higher resultion images
    + The proposed method Flowchart is shown in figure [[fig:meta]].
** The Flowchart of the meta-training phase
#+NAME: fig:meta
#+ATTR_LATEX: :width 12cm :height 6.5cm
 [[~/Dropbox/finalV/diagrams/first/FC.png]]
** GA for optimum freeze layer selection

#+CAPTION: Xception network without GA block converges with a small number of epochs
#+ATTR_LATEX: :width 12cm :height 5.5cm :center
[[~/Dropbox/finalV/diagrams/first/xcepNoGA.png]] 
** GA for optimum freeze layer selection
#+CAPTION: Xception network with GA block converges with a small number of epochs
#+ATTR_LATEX: :width 12cm :height 5.5cm
[[~/Dropbox/finalV/diagrams/first/xcepGA.png]] 

* Full Training
** Ensemble 
   + 4 of the best performing models heterogeneous models are chosen based on their accuracy and misclassification diversity
   + The predictins are stacked horizontally by a flat layer and densely connected to the labels
   + the weights are then trained on the heldout dataset with a much higher resolution 
* Adaptive PSO-Based Ensemble Optimization for Histology Image Classification
  :PROPERTIES:
  :Effort:   3
  :END:
  :LOGBOOK:
  CLOCK: [2021-11-24 Wed 19:39]--[2021-11-24 Wed 20:04] =>  0:25
  CLOCK: [2021-11-24 Wed 17:36]--[2021-11-24 Wed 18:01] =>  0:25
  :END:
** Overview
   + The main objective of Ensemble optimization is finding a robust composite of several classifiers in order to cultivate a single larger and more accurate model
   + The search space of all possible configurations of a stacked ensemble model is massive
   + brute-forcing and grid-search are impractical
   + manual selection of hyperparameter requires experience and doesn't necessarily produce ideal results
** Overview
   + Bio-inspired Computational algorithms proved to be very efficient in solving optimization problems
   + Particle Swarm Optimization (PSO) is one of the most popular techniques used in NP hard problems and optimization methods for non-convex search-space
   + Swarm-based optimization techniques were developed to mimic the social behaviour of groups of animals that are navigating their way cooperatively to find food resources
   + PSO ws first proposed by Eberhart and Kennedy in 1995 as a paradigm for Neural Networks weights optimization
** Overview
   + Its aims is to find the global minimum of objective functions with complex search space
   + PSO starts with initialization of a generation that has a specified number of candidate solutions called particles
   + In each epoch, particles move in the defined finite search space of the problem based on how well the whole swarm of birds is performing
   + The swarm share the best position achieved by the whole swarm, which is the global best position and fitness.
** Overview
   + The velocity and the position of the particle are each updated based on the following equations
\begin{equation}
    V^{t+1}_{ij}=w V^{t}_{ij}+c_1 r^t_1\left(pbest_{ij}-X^t_{ij}\right) +c_2r^t_2\left(gbest_j-X^t_ij\right)
\end{equation}

\begin{equation}
  \begin{gathered}[b]
  x^{t+1}=x^{t}+V^{t+1}
  \end{gathered}
\end{equation}
 - where $V^{t+1}_{ij}$ is the updated velocity, $V^{t}$ is the current velocity 
 - $w$ is the inertia weight, $c1$ and $c2$ are acceleration constants referred to as the cognitive learning rate
 - $gbest$ and  $pbest$ are the global and the population best positions
 - $r1$, $r2$ are random numbers evenly distributed
  between [0,1) cite:He2016.

** PSO advantages
   - Robustness
   - Ease of implementation 
   - Remarkable time efficiency
** Standart PSO limitations
  * PSO is highly reliant on the proper tuning of its learning hyper-parameters
    It is especially sensitive to:
    - inertia $w$ (exploitation vs exploration trade-off)
    - accelerate constants $c1$ and $c2$ (cognitive and social params)
    - $Vmax$ 
    PSO was used to find the best linear combination weights of the horizontally stacked fine-tuned 
     
** Mamdani FIS for adaptive PSO
   * Fuzzy Inference Systems (FIS) were designed to map fuzzy inputs that belong to fuzzy sets with varying degrees to fuzzy outputs
   * FIS has been successfully applied to the adaptation of parameters for numerous algorithms
   * FIS deals with parameters that belong to classes that are not clearly defined
   * A fuzzy set is a set whose elements have a degree of association to one or more fuzzy classes
* Mamdani FIS steps
** fuzzification of the inputs and the outputs
      \begin{equation}
      \begin{gathered}[b]
	  \mu(x,a,b,c) = \max(\min(\frac{x-a}{b-a},\frac{c-x}{c-b}),0)
      \end{gathered}
      \end{equation}
** IF-THEN Rule evaluation
    #+NAME: tbl:rules
    #+CAPTION: The Mamdani Fuzzy Rules
    | Rule number | antecedents                          | consequent |
    |-------------+--------------------------------------+------------|
    |           1 | d1 = high, d2 = high,   NCF = high   | w = high   |
    |           2 | d1 = low , d2 = medium, NCF = medium | w = high   |
    |           3 | d1 = low, d2 = low,   NCF = medium   | w = high   |
    |           4 | d1 = low, d2 = low,   NCF = low      | w = low    |
** Aggregation of the rule output
    
 #+CAPTION: The Mamdani FIS membership functions for the fuzzification of the PSO Inertia Weight
 #+NAME:   fig:fuzz
 [[/home/alkhaldieid/Dropbox/second_final/fuzzy2.png]]
** Defuzzification
 \begin{equation}
   \begin{gathered}[b]
    Center of Gravity = \frac{\sum\limits_{x=a}^{b}\mu_A(x)x}{\sum\limits_{x=a}^{b}\mu_A(x)}
   \end{gathered}
 \end{equation}
 
** Mamdani FIS for adaptive PSO
 #+NAME: fig:alg
#+ATTR_LATEX: :width 12cm :height 5.5cm
 #+CAPTION: Adaptive PSO Algorithm
 [[~/Dropbox/proposal/presentation/pso_algorithm.png]]
* Ensemble Optimization Using Cartesian Genetic Programming
  :PROPERTIES:
  :Effort:   3
  :END:
** Evolutionary Algorithms in ensembles are generally used for:
   1. hyperparameter optimization
   2. topology (design) optimization
** Standard CGP
#+label: fig:cgp
#+CAPTION: The standard CGP representation of the Neural Network and the encoded gene [[cite:&Miller2011]] 
#+ATTR_LATEX: :center textwidth
[[~/Dropbox/third/cgp.png]]
** The CGP method focuses on determining the most optimum architecture of the stacked ensemble
 * Why CGP?
   - because it is more flexible than GA
   - reduces software bloat
 * A solution can be represented in a Cartesian form as shown in Figure [[fig:ens]]
** dCGPANN Variant
#+label: fig:dcgp
#+CAPTION: The dCGP representation of the Neural Network and the additions of weights and biases to the encoded gene [[cite:&Martens2019]] 
#+ATTR_LATEX: :center textwidth
[[~/Dropbox/third/dcgp.png]]
  
** dCGPANN used algorithm
#+NAME: algo
[[~/Dropbox/proposal/presentation/lab/algo.png]]
** Cartesian Genetic Programming
#+label: fig:ens
#+CAPTION: A randomly generated ensemble topology by CGP
#+ATTR_LATEX: :width 12cm :height 5.5cm
[[~/Dropbox/third/ensemble_gene3.png]]
** The proposed method is composed of two stages.
*** Stage 1
   1. fine-tune and train end-to-end multiple heterogeneous
   2. select the best-performing models
   3. The test dataset patches are then augmented to 5000 images.
*** Stage 2
   1. The classifiers' predictions of the 5000 images are used to train the stacked ensemble using dCGPANN.
   2. 

** Cartesian Genetic Programming
 * The predictions of the 5000 images are used to train the stacked ensemble using CGP
 * CGP inputs are a 16x1 vector
 * The solutions are then evolved using crossover and mutation operators based on predefined parameters.
* Ensemble Optimization Using CSA
** Immunecomputing  
   - Colonal Selection Algorithm is bio-inspired by the immune systems in mammals
   - Immune systems has the capacity to identify harmful infected cells (antigens)
   - In response, the immune system produces antibodies to fight antigens
   - Antibodies are then cloned based on the degree of antibodies-antigens recognition score called Affinity Score
  :PROPERTIES:
  :Effort:   3
  :END:
** Parameters to be optimized
   - The antibodies with the highest Affinity scores are used as the antigens for the second epoch
   - The new antigens are then mutated
   - The procedure is repeated for a predefined number of iterations or until stop criteria are met
   - the CSA is going to be used to optimize the hyperparameters of the ensemble training
** Parameters to be optimized
   - each antigen or antibody contains the hyperparameters of the ensmble such as:
     + number of hidden layers
     + number of Convolutional layers
     + number of epochs
     + batch size
     + Type of optimizers
** rephrase                                                        :noexport:
   - The inspiration of natural biological systems is frequently used as a source to solve engineering problems in different domains
   - proposed several different artificial immune models inspired by biological immune system to find global optima for various real-world applications.
   - The human body’s immune system can identify foreign cells that invade the human body, which can be harmful and can cause infections or diseases. These foreign cells are known as Antigens
   - Immune system of human body learns how to cancel the effects of antigens by first understanding and then opposing the behavior pattern of antigen.
   - Cells that are produced by human immune system to fight antigens are called antibodies
   - Affinity is a degree of recognition between antigen and antibody. The higher the affinity, the better the recognition, and vice-versa.
   - CSA uses affinity to select the best antibodies. The best antibodies are cloned [23]. To introduce diversity mutation operation is applied on antibodies with an intent to improve their affinity so that they can destroy the antigen
   - The antibody with best affinity is picked and becomes the antigen for the next iteration.
   - The process is repeated for a fixed number of iterations or until the criterion for problem is achieved.
   - The final antibody is the solution and provides the structure of CNN in terms of hyperparameters. 
* dCGPANN template                                                 :noexport:
** DONE Introduce the use of EA in ensemble optimizatino
** Introduce the GP
** Introduce CGP
** Introduce the dCGP and how it is different that standard CGP
** include the the dCGPANN algorithm
* Experimental Setup
** Evaluation Metrics
      <<first:eval>>
   
       + Accuracy:  the rate of correct predictions to the total number of samples [[cite:&Duchesnay2019;&Javeed2019]].
     \begin{equation}
       acc = \sum TP + \sum TN/\sum \#samples
     \end{equation}
       + Precision: the rate of correct class predictions to the total number of
           samples belonging to that class  [[cite:&Duchesnay2019;&Javeed2019.
     \begin{equation}
      Precision = \sum TP + \sum TN
     \end{equation}

       + Recall: the true positive rate [[cite:&Duchesnay2019;&Javeed2019]].
     \begin{equation}
      TPR = \sum TP /  \#positive \  samples
     \end{equation}
       + F1 score

      \begin{equation}
      F1 score = 2 \times \frac{Precision \times Recall}{(Precision + Recall)}
      \end{equation}
       +  log-loss: the cross-entropy loss, which is the negative log-likelihood of the class labels given predictions [[cite:&Duchesnay2019;&Javeed2019]]
       + The area under the ROC curve
** Dataset 1 BACH: ICIAR 2018 Grand Challenge on *BreAst Cancer Histology* images
     - ICIAR 2018 dataset
     - The histology patches dataset consist of 400 labeled training images 
     - 2048 x 1536 pixels resolution
     - cancerous classes of the images is uniform distribution, as shown in Table [[tab:dataset]]
     - Highly experienced medical specialists classified the images
     - Classes are shown in Table [[tbl:bach]].
** Dataset 1 BACH: ICIAR 2018 Grand Challenge on *BreAst Cancer Histology* images
#+CAPTION: BACH: ICIAR 2018 Grand Challenge on *BreAst Cancer Histology* images
#+LABEL: tbl:bach
+--------+--------+---------+----------+
|   Non-carcinoma |    Carcinoma       |
+--------+--------+---------+----------+
| Normal | Benign | In Situ | Invasive |
+--------+--------+---------+----------+
| 100    |   100  |  100    |  100     | 
+--------+--------+---------+----------+
Unlabeled test images = 100
** Dataset 1 BACH: ICIAR 2018 Grand Challenge on *BreAst Cancer Histology* images
#+NAME:   fig:dataset
#+ATTR_LATEX: :width 12cm :height 5cm
#+CAPTION: Benign, InSitu, Invasive and Normal H&E stained breast samples (from top left to bottom right)
[[/home/alkhaldieid/Dropbox/second_final/data.png]]
** Dataset 1 BACH: ICIAR 2018 Grand Challenge on *BreAst Cancer Histology* images
#+CAPTION: The Distribution of the ICIAR Dataset in our Study.
#+NAME: tab:dataset
| Type          | Normal | Benign | InSitu | Invasive |
| Train         |     90 |     90 |     90 |       90 |
| validation    |      5 |      5 |      5 |        5 |
| Held-out test |      5 |      5 |      5 |        5 |
| Test          |     25 |     25 |     25 |       25 |
** Dataset 2 IDC
 * Extracted from 279 patients
 * 277524 non-overlapping 50x50 patches
 * Binary Classes
** Dataset 2 IDC statistics
#+CAPTION: Dataset Statistics
#+LABEL: fig:bhistats
 [[~/Dropbox/third/datasethists.png]]
  
** Dataset 2 IDC Positive Samples
#+CAPTION: IDC positive samples 
#+LABEL: fig:idcpp
[[~/Dropbox/third/idcpp.png]]
** Dataset 2 IDC Negative Samples
#+CAPTION: IDC negative samples 
#+LABEL: fig:idcnp
[[~/Dropbox/third/idcnp.png]]
** Hardware and Sofrware
 * The experiments are run on:
   - NVIDIA Geo-Force RTX 2080 GPU
   - AMD Ryzen Threadripper 1950X 16-Core processor
 * Using the following open source software
   - Tensorflow
   - Keras
   - Pyton-dcgp
   - DEAP
   - gplearn
   - PySwarm
   - Artix Linux
   - NEAT
*** rephrase                                                       :noexport:
The GA begins by creating a random population of weight distribution vectors. The evaluation of
each population member is done by evaluating the corresponding meta-classifier created by
using its weight distribution vector on the holdout subset. The fitness of each member is then
calculated to be the prediction accuracy of that meta-classifier on the holdout subset. Using the
fitness of each population member, the GA then performs the tournament selection to select
members for the next generation. It then applies the crossover and mutation operators to create a
new generation of weight distribution vectors. The above process is repeated for 3000
generations, and the best weight distribution vector from its final population is selected to create
the meta-classifier. cite:Sikora2016 
* Suggested Dissertation Outline

#+CAPTION: The anticipated dissertation contents part 1
|---------+----------------------------------------------|
| Chapter | Title                                        |
|---------+----------------------------------------------|
|      1. | Introduction                                 |
|---------+----------------------------------------------|
|      2. | Literature Review                            |
|---------+----------------------------------------------|
|      3. | Genetically Optimized Heterogeneous Ensemble |
|         | for Histological Image Classification        |
|---------+----------------------------------------------|
|      4. | Adaptive PSO-Based Ensemble Optimization     |
|         | for Histology Image Classification           |
|---------+----------------------------------------------|
|      5. | Cartesian Genetic Programming                |
|         | for Stacked Ensemble Optimization            |
|---------+----------------------------------------------|
|      6. | Ensemble Training Optimization               |
|         | Using Colonal Selection Algorithm            |
|---------+----------------------------------------------|
|      8. | Conclusion and Future Work                   |
|---------+----------------------------------------------|
                                                       
** Dissertation-related Research Publications
     [1] Alkhaldi, E. & Salari, E. (2019) Genetically Optimized
     Heterogeneous Ensemble for Histological Image
     Classification. International Journal of Science and
     Engineering Investigations (IJSEI), 8(95), 113-118.

     [2] E. Alkhaldi and E. Salari, “Adaptive PSO-Based Ensemble Optimization for Histology
     Image Classification,”. International Journal of Computer Science and Technology (IJCST), vol. 8491, pp. 26–34, 2021.
* Academic Progress and Research Timeline                          :noexport:
** GANTT Chart
#+CAPTION: 2017-2018 GANNT Chart
[[~/Dropbox/proposal/images/GANNT1.png]]

#+CAPTION: 2019-2020 GANNT Chart
[[~/Dropbox/proposal/images/GANNT2.png]]

#+CAPTION: 2021 GANNT Chart
[[~/Dropbox/proposal/images/GANNT3.png]]

* Questions?
  Questions?
  
\begin{frame}[allowframebreaks]
        \frametitle{References}
        \bibliographystyle{ieeetr}
        \bibliography{/home/alkhaldieid/work/res/cited_lib.bib}
\end{frame}
