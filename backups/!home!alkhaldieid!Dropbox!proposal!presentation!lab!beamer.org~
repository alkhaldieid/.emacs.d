#+TITLE: Ensemble Optimization for Histological Image Classification
#+Author: Eid Alkhaldi
#+Email: eid.alkhaldi@gmail.com
#+LATEX_CLASS: beamer
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage{algorithmic}
#+LATEX_HEADER: \usepackage{algpseudocode}


#+EXCLUDE_TAGS: noexport
#+OPTIONS: H:2 toc:nil num:nil
#+BEAMER_THEME: Antibes
#+PROPERTY:  header-args :eval no

* Motivation and Challenges
** Importance of Breast Histological image classification
    * Breast cancer is the most common type of cancer amongst women worldwide
    * More than 10% of women in the U.S. will be diagnosed with breast cancer 
    * Microscopic analysis of biopsy is the most accurate diagnosis method
    * Demands extensive work
** Importance of Breast Histological image classification
    * Requires specialized expertise (pathologists)
    * Time consuming due to scarcity of pathologists
    * Expensive to hire 
    * Pathologists often disagree
** Importance of Breast Histological image classification
 Early diagnosis is important for:
  - Prognosis
  - Reducing healthcare costs
  - Better chance of recovery
** Importance of Breast Histological image classification
 * An automated intelligent approach is needed to compensate for the time delay of analysis caused by the insufficient number of pathologists 
 * The aim of histology image classification methods is to identify abnormalities in the specimens' structures and specify their carcinogenic level
** Challenges 
 - Deep Learning (DL) showed remarkable competence
   + Extract complex features
   + Expands class separability better than conventional ML
   + CNNs are the most reliable DL models
- Limitations:
  + CNNs has a huge number of parameters
  + It demands a high quantity of labeled images
  + Curse of Dimensionality
    * The larger the dataset, the more training time is required
** Challenges 
  + Consequently, end-to-end training of CNNs suffered significantly, due to the scarcity of annotated hematoxylin and eosin (H&E) stained histology images.
  + End-to-end training is data greedy
  + End-to-end training using an insufficient number of classified examples is prone to overfitting regardless of how the weights were initialized
** Challenges 
- Transfer learning is the reusing of a model trained on a large dataset for a different task
- The general layers learns general features
- Training only the task specific layers is very efficient
** Challenges 
- Transfer Learning proved to be a more reliable alternative to  end-to-end training
  + Improves accuracy and robustness
  + Reduces the training time significantly, particularly for small datasets
- Limitations:
  + Could result in overfitting, mainly when training the general layers
  + It lacks the ability to separate task-specific layers from general ones
** Challenges 
- Ensembles are a vertical or cascaded combination of the features or the predictions of accurate models to produce a better performing model
  + Ensembling a number of models proved to be very effective and more robust alternative
  + For ensembles to perform better than its individual model, the models have to be heterogeneous
    * Why?
      - To avoid making the same misclassifications
* Proposed Solutions	
** Proposed Solutions
    * Genetically Optimized Heterogeneous Ensemble for Histolgy Image Classification
    * Adaptive PSO-Based Ensemble Optimization for Histology Image Classification
    * Ensemble Optimization Using Cartesian Genetic Programming
    * Ensemble Optimization Using Colonal Selection Algorithm
* Genetically Optimized Heterogeneous Ensemble for Histolgy Image Classification
** Meta-training 
   The first proposed methods is composed of two phases:
    + Meta-training
    + Ensemble training
   The meta-training is composed of three stages:
      + Preprocessing
      + Transfer Learning
** Preprocessing
   + The aim of preprocessing is to augment the dataset
   + The histological datasets are usually small due to the lack of data
   + Affine Transformation applied
     * random cropping
     * rotation
     * width/height shifting
     * vertical/horizontal flipping
   + Resizeing and cropping to a lower resoluiton
   + Normalization
* Transfer Learning
** Introduction 
   + Transfer Learning is the process of re-utilization of models' weights that were trained on benchmark datasets such as ImageNet
     * More reliable, robust and accurate than end-to-end
     * More time-efficient
   + The weights of a model that was trained end-to-end is used as the initialization for the training of images in a different domain
   + For the new domain, the parameters of the domain-specific layers only are trainable
   + All other weights are frozen
    
** Transfer Learning Last layers 

| Layer                    | Shape        | trainable params. |
|--------------------------+--------------+-------------------|
| global_average_pooling2d | (None, 2048) |                 0 |
| gaussian_dropout         | (None, 2048) |                 0 |
| dense                    | (None, 4)    |              8196 |
** Gaussian Dropout
 \begin{equation} \label{gaus} \sigma _(gaussian)= \sqrt{rate/(1 - rate)}
   \end{equation} 
   Where /rate/ denotes the user control parameter
   and \(\sigma _(gaussian)\) refers the dropout gaussian standard deviation
+ Why?
  +  Blocking several units from firing during the feedforward and the backpropagation steps of training
+ restricts the classifier from learning the irrelevant particularities of an image
 
** Learning Rate Scheduler
     Why LRS?
     + Aims to minimize the cost function
     + Avoids saddle points in the search spaces which are common in high dimensional problems when LR is too small
     + However, over-increasing the LR will cause fluctuations with high spikes as shown in figure [[fig:LR]]
** Cyclical Learning Rates
#+NAME: fig:LR
#+ATTR_LATEX: :center :width 12cm :height 4cm
#+CAPTION: High spikes of validiation loss and accuracy curves over exponential cyclical learning rate in a single epoch in ResNet50 with freeze layer = 141 and a batch size = 32. The learning rate with the lowest validation loss at lr = 0.05 is chosen as the base lr for the full training.
[[/home/alkhaldieid/Dropbox/finalV/diagrams/first/cyc.png]]
** One Cycle Policy
Triangular Cyclical Learning Rates and illustrated by smith in figure [[fig:tri]]. 
#+NAME: fig:tri
#+Caption: Smith's Triangular Cyclical Learning Rates Method.
[[~/repos/writings/proposal/paper/second_draft/tri.png]]

** GA for optimum freeze layer selection
    + The objective of implementing the genetic algorithm is to evolve the solutions to the fittest task-specific layer 
    + GA is an optimization method inspired by simulating the natural selection through crossover and mutation cite:Singh2018
    + The possible layers can't be batch normalizaiton layers or the first 20% of layers
    + Each possible layer is encoded by a binary chromosome
    + After calculating the initial population fitnesses, the fittest layers are chosen for crossover at a predefined crossover probability Pc to produce better offsprings in the next generation.
    + All layers, except BN, are frozen upto the decoded layer
** GA for optimum freeze layer selection
    + cyclical LR is implemented to pick the LR that is most appropriate for the decoded freeze-layer
    + The model is then trained on lower resolution images for 10 epochs
    + The cross-entropy loss is then calculated on the validation dataset
    + The fittest model is then trained thoroughly with higher resultion images
#    + The proposed method Flowchart is shown in figure [[fig:meta]].
** The Flowchart of the meta-training phase                       :noexport:
#+NAME: fig:meta
#+ATTR_LATEX: :width 12cm :height 6.5cm
 [[~/Dropbox/finalV/diagrams/first/FC.png]]
** GA for optimum freeze layer selection

#+CAPTION: Xception network without GA block converges with a small number of epochs
#+ATTR_LATEX: :width 12cm :height 5.5cm :center
[[~/Dropbox/finalV/diagrams/first/xcepNoGA.png]] 
** GA for optimum freeze layer selection
#+CAPTION: Xception network with GA block converges with a small number of epochs
#+ATTR_LATEX: :width 12cm :height 5.5cm
[[~/Dropbox/finalV/diagrams/first/xcepGA.png]] 
** Confusion Matrices of the best performing models.

#+NAME: fig:confs
#+CAPTION: Error Type Visualization aim to choose hetrogenous models using the confusion matrix
#+ATTR_LATEX: :width 0.45\textwidth
[[~/Dropbox/proposal/presentation/lab/confs.png]]

* Full Training
** Ensemble 
   + 4 of the best performing models heterogeneous models are chosen based on their accuracy and misclassifications diversity
   + The predictions are stacked horizontally by a flat layer and densely connected to the labels
   + the weights are then trained on the holdout dataset with a much higher resolution 

* Experimental Setup
** Dataset
The images are classified according to their predominant cancer type into 4 classes:
   
#+NAME: tbl:bach
#+CAPTION: BACH: ICIAR 2018 Grand Challenge on *BreAst Cancer Histology* images
+--------+--------+---------+----------+
|   Non-carcinoma |    Carcinoma       |
+--------+--------+---------+----------+
| Normal | Benign | In Situ | Invasive |
+--------+--------+---------+----------+
| 100    |   100  |  100    |  100     | 
+--------+--------+---------+----------+
** For training
#+NAME: tbl:dataset
#+CAPTION: The Distribution of the ICIAR Dataset 
| Type          | Normal | Benign | InSitu | Invasive |
|---------------+--------+--------+--------+----------|
| Train         |     90 |     90 |     90 |       90 |
| validation    |      5 |      5 |      5 |        5 |
| Held-out test |      5 |      5 |      5 |        5 |
| Test          |     25 |     25 |     25 |       25 |
** Models The constructed the ensemble
#+NAME: tbl:gamodels
#+CAPTION: The models used to construct the ensemble.
|  # | Model Name        | namber of parameters | number of layers |
| 1. | DenseNet201       | 20.2M                |              402 |
| 2. | ResNet50          | 25.6M                |              107 |
| 3. | InceptionResnetv2 | 55.9M                |              449 |
| 4. | Xception          | 22.9M                |               81 |
** Evaluation Metrics
  * Accuracy:  the ratio of correct predictions to the total number of samples [[cite:&Duchesnay2019]]  
\begin{equation}
  acc =  \frac{TP +  TN}{\#samples}
\end{equation}
  * Precision: the rate of correct class predictions to the total number of
      samples belonging to that class  [[cite:&Duchesnay2019]] 

#+NAME: eqn:precision1
\begin{equation}
Precision = \frac{TP}{TP+ FN}
\end{equation}

   * Recall: the true positive rate [[cite:&Duchesnay2019]] 

#+NAME: eq:recall1
\begin{equation}
Precision = \frac{TP}{TP+ FN}
\end{equation}

** Evaluation Metrics
   * F1 score which is defined by the following equation: 
   \begin{equation}
   F1 score = 2 \times \frac{Precision \times Recall}{(Precision + Recall)}
   \end{equation}
   * log-loss: the cross-entropy loss, which is the negative log-likelihood of the class labels given predictions [[cite:&Duchesnay2019]]
   * Macro-average: is the sum of each class' precision divided by the number of classes.
   * Micro-average: is the sum of the true positives of all classes divided by the number of samples.
   * Area under the ROC curve.

** Results

#+NAME: maxcr
#+CAPTION: Maximum Prediction voting classification report
| {Class. Report} | precision | recall | f1-score |
| Benign          |      1.00 |   0.37 |     0.54 |
| InSitu          |      0.60 |   1.00 |     0.75 |
| Invasive        |      1.00 |   0.93 |     0.97 |
| Normal          |      0.97 |   1.00 |     0.98 |

** Results
#+NAME: tab:avgcr
#+CAPTION:  Average Voting Classification Report
| {Class. Report} | precision | recall | f1-score |
| Benign          |      1.00 |   0.70 |     0.82 |
| InSitu          |      0.97 |   1.00 |     0.98 |
| Invasive        |      1.00 |   1.00 |     1.00 |
| Normal          |      0.79 |   1.00 |     0.88 |

** Results
#+NAME: tab:prpcr1
#+CAPTION:  Proposed Voting Classification Report
| {Class. Report} | precision | recall | f1-score |
| Benign          |      1.00 |   0.73 |     0.85 |
| In Situ         |      0.97 |   1.00 |     0.98 |
| Invasive        |      1.00 |   1.00 |     1.00 |
| Normal          |      0.81 |   1.00 |     0.90 |

** Results

#+NAME: tab:prpcr2
#+CAPTION: Evaluation of The Proposed Method Compared to Maximum and Average voting
| Eval. /method                |      Proposed |   Max. |   Avg. |
| Probabilities auc roc        |        0.9994 |    1.0 | 0.9934 |
| Labels auc roc               |       0.95556 | 0.8833 |   0.95 |
| Labels accuracy              |         0.933 |  0.825 |  0.925 |
| Probabilities precision      |        0.9985 |   0.94 |  0.988 |
| Labels precision             |        0.8946 |  0.760 |  0.883 |
| Labels log loss              |         2.302 | 6.0442 |  2.590 |
| ICIAR acc.                   | \textbf{88\%} |    85% |    87% |
* Adaptive PSO-Based Ensemble Optimization for Histology Image Classification
  :PROPERTIES:
  :Effort:   3
  :END:
  :LOGBOOK:
  CLOCK: [2021-11-24 Wed 19:39]--[2021-11-24 Wed 20:04] =>  0:25
  CLOCK: [2021-11-24 Wed 17:36]--[2021-11-24 Wed 18:01] =>  0:25
  :END:
** Overview
   + The main objective of Ensemble optimization is finding a robust composite of several classifiers in order to cultivate a single larger and more accurate model
   + The search space of all possible configurations of a stacked ensemble model is massive
   + brute-forcing and grid-search are impractical
   + manual selection of hyperparameter requires experience and doesn't necessarily produce ideal results
**  Conventional Ensemble Methods Disadvantages
Majority and Maximum voting
 * assumes the optimum weights of the classifiers to be equal
 * This assumption undermines the complexity of medical applications
 But, in histological image are classified into their predominant cancer type!
 This leads to classification more biased toward abnormal labels!

**  Meta Training
 * Transfer Learning Last layers 
| Layer                    | Shape        | trainable params. |
|--------------------------+--------------+-------------------|
| global_average_pooling2d | (None, 2048) |                 0 |
| gaussian_dropout         | (None, 2048) |                 0 |
| dense                    | (None, 4)    |              8196 |
 * RandAugment for data augmentation
   * why Rand-Augment?
 * One Policy Strategy to find proper initial LR
** PSO
   + Bio-inspired Computational algorithms proved to be very efficient in solving optimization problems
   + Particle Swarm Optimization (PSO) is one of the most popular techniques used in NP hard problems and optimization methods for non-convex search-space
   + Swarm-based optimization techniques were developed to mimic the social behaviour of groups of animals that are navigating their way cooperatively to find food resources
   + PSO ws first proposed by Eberhart and Kennedy in 1995 as a paradigm for Neural Networks weights optimization
** PSO
   + Its aims is to find the global minimum of objective functions with complex search space
   + PSO starts with initialization of a generation that has a specified number of candidate solutions called particles
   + In each epoch, particles move in the defined finite search space of the problem based on how well the whole swarm of birds is performing
   + The swarm share the best position achieved by the whole swarm, which is the global best position and fitness.
** PSO 
 * PSO history
 * Swarm-based optimization
 * Mimics social behaviour of a group animals
 * Eberhart and Kennedy [[cite:&Eberhart1995]]
** NN encoding for PSO
 * each particle of the PSO is composed of the weights and the biases of the Neural Network that fuse the predictions of the ensemble models. 
 * The feed-forward NN is constructed as follows:
    1. The input layer is a [1x16] vector which represents the output predictions of the individual models.
    2. The second layer is densely connected to the input layer and to $b1$ bias term.
    3. The $W1$ vector holds the [1x128] weights that connects the input layer to the hidden layer.
    4. $W2$ contains the weights connecting the hidden layer to the output.
    5. The output of the hidden layer is the output $a1$ of the hyperbolic tan applied on the logits  vector defined in equation [[eqn:tanh]]. 

** NN encoding for PSO
#+NAME: eqn:tanh
 \begin{equation}
  a1 = tanh(X \cdot W1 + b1)
 \end{equation}
 
The output of the last layer is the final predictions vector whose loss is optimized over the validation set and is defined by equation [[eqn:output]]. 

#+NAME: eqn:output
\begin{equation}
 y = softmax(a1 \cdot W2 + b2)
\end{equation}

Where softmax is an activation function that maps the logits into a probability distribution as shown in equation [[eqn:softmax]] [[cite:&Nwankpa2018]]. 

 #+NAME: eqn:softmax
\begin{equation}
 f(x_i) = \frac{\exp(x_i)}{\sum_j(exp(x_j))}
\end{equation}
 
** PSO particle
The proposed NN is shown in Figure [[fig:pso_nn]].
 * Each particle $P_i$ in the PSO swarm represents the weights and the biases of the NN shown in [[fig:pso_nn]].
   $P_i$ = [W1,b1,W2,b2] which is a [1x172].
   
 * The categorical cross entropy is the objective function that the PSO is minimizing and is defined in [[eqn:loss]] [[cite:&patterson2017]].
 
#+NAME: eqn:loss
\begin{equation}
  Loss = - \sum_{i=1}^N \sum_{j=1}^C y_{i,j} \times \log \hat{y_{i,j}}
\end{equation}
** PSO particle
#+NAME:  fig:pso_nn 
#+ATTR_LATEX: :width 0.3\textwidth
#+CAPTION: The proposed NN that combine the predictions of the four models to the final prediction
[[~/Dropbox/proposal/presentation/lab/psoNN.png]]

 
   
** PSO Velocity and Position update
The velocity and the position of the particle are each updated based on the equation [[eqn:velocity]] and [[eqn:position]]. 
#+NAME: eqn:velocity
\begin{equation}
    V^{t+1}_{i}=w V^{t}_{i}+c_1 r_1\left(X_{pbest_{i}}-X^t_{i}\right) +c_2r_2\left(X_{gbest}-X^t_i\right)
\end{equation}

#+NAME: eqn:position
\begin{equation}
   x_i^{t+1}=x_i^{t}+V_i^{t+1}
\end{equation}

** PSO Velocity and Position update
 - Where $V^{t+1}_{i}$ is the updated velocity for particle i,
 - $V_i^{t}$ is its current velocity,
 - $w$ is the inertia weight,
 - $c1$ and $c2$ are acceleration constants referred to as the cognitive learning rate,
 - $X_{gbest}$ and $X_{pbest}_i$ are the global and the personal best positions respectively, and
 - $r1$, $r2$ are random numbers evenly distributed \in [0,1] cite:He2016.

** PSO advantages
   - Robustness
   - Ease of implementation 
   - Remarkable time efficiency
** Standart PSO limitations
  * PSO is highly reliant on the proper tuning of its learning hyper-parameters
    It is especially sensitive to:
    - inertia $w$ (exploitation vs exploration trade-off)
    - accelerate constants $c1$ and $c2$ (cognitive and social params)
** How $w$ controls the convergence of PSO
*** Exploration vs. Exploitation
- regulates degree of the influence that the earlier velocity has on the updated velocity
- Therefore, it controls the speed at which a particle moves in the search space.
- The larger the inertia weight is, the faster the particle moves and the less likely it will be to get trapped on a local minimum
- However it could escape the local minimum if it is too large
** Mamdani FIS for adaptive PSO
   * Fuzzy Inference Systems (FIS) were designed to map fuzzy inputs that belong to fuzzy sets with varying degrees to fuzzy outputs
   * FIS has been successfully applied to the adaptation of parameters for numerous algorithms
   * FIS deals with parameters that belong to classes that are not clearly defined
   * A fuzzy set is a set whose elements have a degree of association to one or more fuzzy classes
* Mamdani FIS steps
** fuzzification of the inputs and the outputs
      \begin{equation}
      \begin{gathered}[b]
	  \mu(x,a,b,c) = \max(\min(\frac{x-a}{b-a},\frac{c-x}{c-b}),0)
      \end{gathered}
      \end{equation}
** IF-THEN Rule evaluation
#+NAME: tbl:rules
#+CAPTION: The Mamdani Fuzzy Rules
| Rule number | antecedents                          | consequent |
|-------------+--------------------------------------+------------|
|           1 | d1 = high, d2 = high,   NCF = high   | w = high   |
|           2 | d1 = low , d2 = medium, NCF = medium | w = high   |
|           3 | d1 = low, d2 = low,   NCF = medium   | w = high   |
|           4 | d1 = low, d2 = low,   NCF = high     | w = high   |
|           5 | d1 = low, d2 = low,   NCF = low      | w = low    |
** Rules intuition 
  * When all the metrics are low, the optimal solution is close.
       Therefore, the value of $w$ is set to low in order to increase the exploitation to refine the local search.
  * Otherwise, the region of interest is not near yet.
       As a result, the exploration is increased by giving a high value for the inertia $w$. 
 Figure [[fig:fuzz]] shows the fuzzy membership functions of the antecedents and the consequents rules of the FIS. 
** Rule Evaluation
#+CAPTION: The Mamdani FIS membership functions for the fuzzification of the PSO Inertia Weight
#+ATTR_LATEX:   :width 12cm :height 5.5cm
#+NAME:   fig:fuzz
 [[/home/alkhaldieid/repos/writings/proposal/paper/second_draft/fuzzy.png]]
** Aggregation of the rule output
The Aggregate output of the rule evaluation is a scaled and cropped version of the fuzzy inputs 
    
** Defuzzification
The crisp output of the FIS is calculated using equation [[eqn:gravity]].
#+NAME: eqn:gravity
\begin{equation}
   Center\; of\; Gravity (COG) = \frac{\sum\limits_{x=a}^{b}\mu_A(x)x}{\sum\limits_{x=a}^{b}\mu_A(x)}
\end{equation}
** Mamdani FIS for adaptive PSO
 #+NAME: fig:alg
#+ATTR_LATEX: :width 12cm :height 5.5cm
 #+CAPTION: Adaptive PSO Algorithm
 [[~/Dropbox/proposal/presentation/lab/pso_alg.png]]
** Models that constructed the ensmble
#+NAME: tbl:psomodels
#+CAPTION: The models used to construct the ensemble.
|  # | Model Name  | number of parameters | number of layers |
| 1. | InceptionV3 | 23.9M                |              189 |
| 2. | NasNetLarge | 5.3M                 |              389 |
| 3. | DenseNet201 | 20.2M                |              402 |
| 4. | ResNet152V2 | 60.4M                |              107 |
** Results
 Table [[tbl:prpso2]] show that our approach outperformed the weighted average method by 2% accuracy. 

 #+NAME: tbl:maxpso
 #+CAPTION:  Maximum Voting Classification Report
 #+ATTR_LATEX: :center t :placement [H]
 | {Class. Report} | precision | recall | F1-score |
 |-----------------+-----------+--------+----------|
 | Benign          |      1.00 |   0.37 |     0.54 |
 | InSitu          |      0.60 |   1.00 |     0.75 |
 | Invasive        |      1.00 |   0.93 |     0.97 |
 | Normal          |      0.97 |   1.00 |     0.98 |

** Results
#+NAME: tbl:prpso
#+CAPTION: Proposed Voting Classification Report
#+ATTR_LATEX: :center t :placement [H]
| {Class. Report} | precision | recall | F1-score |
|-----------------+-----------+--------+----------|
| Benign          |      1.00 |   0.87 |     0.95 |
| In Situ         |      0.98 |   0.99 |     0.98 |
| Invasive        |      0.99 |   0.99 |     0.99 |
| Normal          |      0.92 |   0.94 |     0.91 |

** Results

#+NAME: tbl:prpso2
#+CAPTION:  Evaluation of The Proposed Method Compared to Average voting
#+ATTR_LATEX: :center t :placement [H]
| Eval. metric      | Proposed |   Avg. |
|-------------------+----------+--------|
| P. AUC ROC        |   0.9983 | 0.9934 |
| L. AUC ROC        |    0.966 |   0.95 |
| L. accuracy       |    0.951 |  0.925 |
| P. precision      |     0.99 |  0.988 |
| L. precision      |   0.9033 |  0.883 |
| L. log loss       |    2.108 |  2.590 |
| ICIAR acc.        |      89% |    87% |
 Where /P. AUC ROC/ stands for the area under the ROC curve for the output probabilities, while /L. AUC ROC/ represents the area under the ROC curve for the one hot encoding for the labels produced by choosing 1 for the class with highest probability and 0 otherwise. 

* Ensemble Optimization Using Cartesian Genetic Programming
  :PROPERTIES:
  :Effort:   3
  :END:
** Evolutionary Algorithms in ensembles are generally used for:
   1. hyperparameter optimization
   2. topology (design) optimization
** Standard CGP
#+label: fig:cgp
#+CAPTION: The standard CGP representation of the Neural Network and the encoded gene [[cite:&Miller2011]] 
#+ATTR_LATEX: :center textwidth
[[~/Dropbox/third/cgp.png]]

** The CGP method focuses on determining the most optimum architecture of the stacked ensemble
 * Why CGP?
   - because it is more flexible than GA
   - reduces software bloat
 * A solution can be represented in a Cartesian form as shown in Figure [[fig:ens]]
** dCGPANN Variant
#+label: fig:dcgp
#+CAPTION: The dCGP representation of the Neural Network and the additions of weights and biases to the encoded gene [[cite:&Martens2019]] 
#+ATTR_LATEX: :center textwidth
[[~/Dropbox/third/dcgp.png]]
  
** dCGPANN used algorithm
#+NAME: algo
[[~/Dropbox/proposal/presentation/lab/algo.png]]
** Cartesian Genetic Programming
#+label: fig:ens
#+CAPTION: A randomly generated ensemble topology by CGP
#+ATTR_LATEX: :width 12cm :height 5.5cm
[[~/Dropbox/third/ensemble_gene3.png]]
** The proposed method is composed of two stages.
*** Stage 1
   1. fine-tune and train end-to-end multiple heterogeneous
   2. select the best-performing models
   3. The test dataset patches are then augmented to 5000 images.
*** Stage 2
   1. The classifiers' predictions of the 5000 images are used to train the stacked ensemble using dCGPANN.
   2. the inputs of the dCGPANN is [8x1] vector which is the output of the individual models

** Cartesian Genetic Programming
 * The predictions of the 5000 images are used to train the stacked ensemble using CGP
 * CGP inputs are a 16x1 vector
 * The solutions are then evolved using crossover and mutation operators based on predefined parameters.
* Ensemble Optimization Using CSA
** Immunecomputing  
   - Colonal Selection Algorithm is bio-inspired by the immune systems in mammals
   - Immune systems has the capacity to identify harmful infected cells (antigens)
   - In response, the immune system produces antibodies to fight antigens
   - Antibodies are then cloned based on the degree of antibodies-antigens recognition score called Affinity Score
  :PROPERTIES:
  :Effort:   3
  :END:
** Parameters to be optimized
   - The antibodies with the highest Affinity scores are used as the antigens for the second epoch
   - The new antigens are then mutated
   - The procedure is repeated for a predefined number of iterations or until stop criteria are met
   - the CSA is going to be used to optimize the hyperparameters of the ensemble training
** Parameters to be optimized
   - each antigen or antibody contains the hyperparameters of the ensmble such as:
     + number of hidden layers
     + number of Convolutional layers
     + number of epochs
     + batch size
     + Type of optimizers
** rephrase                                                        :noexport:
   - The inspiration of natural biological systems is frequently used as a source to solve engineering problems in different domains
   - proposed several different artificial immune models inspired by biological immune system to find global optima for various real-world applications.
   - The human body’s immune system can identify foreign cells that invade the human body, which can be harmful and can cause infections or diseases. These foreign cells are known as Antigens
   - Immune system of human body learns how to cancel the effects of antigens by first understanding and then opposing the behavior pattern of antigen.
   - Cells that are produced by human immune system to fight antigens are called antibodies
   - Affinity is a degree of recognition between antigen and antibody. The higher the affinity, the better the recognition, and vice-versa.
   - CSA uses affinity to select the best antibodies. The best antibodies are cloned [23]. To introduce diversity mutation operation is applied on antibodies with an intent to improve their affinity so that they can destroy the antigen
   - The antibody with best affinity is picked and becomes the antigen for the next iteration.
   - The process is repeated for a fixed number of iterations or until the criterion for problem is achieved.
   - The final antibody is the solution and provides the structure of CNN in terms of hyperparameters. 
* dCGPANN template                                                 :noexport:
** DONE Introduce the use of EA in ensemble optimizatino
** Introduce the GP
** Introduce CGP
** Introduce the dCGP and how it is different that standard CGP
** include the the dCGPANN algorithm
* Experimental Setup
** Evaluation Metrics
      <<first:eval>>
   
       + Accuracy:  the rate of correct predictions to the total number of samples [[cite:&Duchesnay2019;&Javeed2019]].
     \begin{equation}
       acc = \sum TP + \sum TN/\sum \#samples
     \end{equation}
       + Precision: the rate of correct class predictions to the total number of
           samples belonging to that class  [[cite:&Duchesnay2019;&Javeed2019.
     \begin{equation}
      Precision = \sum TP + \sum TN
     \end{equation}

       + Recall: the true positive rate [[cite:&Duchesnay2019;&Javeed2019]].
     \begin{equation}
      TPR = \sum TP /  \#positive \  samples
     \end{equation}
       + F1 score

      \begin{equation}
      F1 score = 2 \times \frac{Precision \times Recall}{(Precision + Recall)}
      \end{equation}
       +  log-loss: the cross-entropy loss, which is the negative log-likelihood of the class labels given predictions [[cite:&Duchesnay2019;&Javeed2019]]
       + The area under the ROC curve
** Dataset 1 BACH: ICIAR 2018 Grand Challenge on *BreAst Cancer Histology* images
     - ICIAR 2018 dataset
     - The histology patches dataset consist of 400 labeled training images 
     - 2048 x 1536 pixels resolution
     - cancerous classes of the images is uniform distribution, as shown in Table [[tab:dataset]]
     - Highly experienced medical specialists classified the images
     - Classes are shown in Table [[tbl:bach]].
** Dataset 1 BACH: ICIAR 2018 Grand Challenge on *BreAst Cancer Histology* images
#+CAPTION: BACH: ICIAR 2018 Grand Challenge on *BreAst Cancer Histology* images
#+LABEL: tbl:bach
+--------+--------+---------+----------+
|   Non-carcinoma |    Carcinoma       |
+--------+--------+---------+----------+
| Normal | Benign | In Situ | Invasive |
+--------+--------+---------+----------+
| 100    |   100  |  100    |  100     | 
+--------+--------+---------+----------+
Unlabeled test images = 100
** Dataset 1 BACH: ICIAR 2018 Grand Challenge on *BreAst Cancer Histology* images
#+NAME:   fig:dataset
#+ATTR_LATEX: :width 12cm :height 5cm
#+CAPTION: Benign, InSitu, Invasive and Normal H&E stained breast samples (from top left to bottom right)
[[/home/alkhaldieid/Dropbox/second_final/data.png]]
** Dataset 1 BACH: ICIAR 2018 Grand Challenge on *BreAst Cancer Histology* images
#+CAPTION: The Distribution of the ICIAR Dataset in our Study.
#+NAME: tab:dataset
| Type          | Normal | Benign | InSitu | Invasive |
| Train         |     90 |     90 |     90 |       90 |
| validation    |      5 |      5 |      5 |        5 |
| Held-out test |      5 |      5 |      5 |        5 |
| Test          |     25 |     25 |     25 |       25 |
** Dataset 2 IDC
 * Extracted from 279 patients
 * 277524 non-overlapping 50x50 patches
 * Binary Classes
** Dataset 2 IDC statistics
#+CAPTION: Dataset Statistics
#+LABEL: fig:bhistats
 [[~/Dropbox/third/datasethists.png]]
  
** Dataset 2 IDC Positive Samples
#+CAPTION: IDC positive samples 
#+LABEL: fig:idcpp
[[~/Dropbox/third/idcpp.png]]
** Dataset 2 IDC Negative Samples
#+CAPTION: IDC negative samples 
#+LABEL: fig:idcnp
[[~/Dropbox/third/idcnp.png]]
** Hardware and Software
 * The experiments are run on:
   - NVIDIA Geo-Force RTX 2080 GPU
   - AMD Ryzen Threadripper 1950X 16-Core processor
 * Using the following open source software
   - Tensorflow
   - Keras
   - Pyton-dcgp
   - DEAP
   - gplearn
   - PySwarm
   - Artix Linux
   - NEAT
*** rephrase                                                       :noexport:
The GA begins by creating a random population of weight distribution vectors. The evaluation of
each population member is done by evaluating the corresponding meta-classifier created by
using its weight distribution vector on the holdout subset. The fitness of each member is then
calculated to be the prediction accuracy of that meta-classifier on the holdout subset. Using the
fitness of each population member, the GA then performs the tournament selection to select
members for the next generation. It then applies the crossover and mutation operators to create a
new generation of weight distribution vectors. The above process is repeated for 3000
generations, and the best weight distribution vector from its final population is selected to create
the meta-classifier. cite:Sikora2016 
* Suggested Dissertation Outline

#+CAPTION: The anticipated dissertation contents part 1
|---------+----------------------------------------------|
| Chapter | Title                                        |
|---------+----------------------------------------------|
|      1. | Introduction                                 |
|---------+----------------------------------------------|
|      2. | Literature Review                            |
|---------+----------------------------------------------|
|      3. | Genetically Optimized Heterogeneous Ensemble |
|         | for Histological Image Classification        |
|---------+----------------------------------------------|
|      4. | Adaptive PSO-Based Ensemble Optimization     |
|         | for Histology Image Classification           |
|---------+----------------------------------------------|
|      5. | Cartesian Genetic Programming                |
|         | for Stacked Ensemble Optimization            |
|---------+----------------------------------------------|
|      6. | Ensemble Training Optimization               |
|         | Using Colonal Selection Algorithm            |
|---------+----------------------------------------------|
|      8. | Conclusion and Future Work                   |
|---------+----------------------------------------------|
                                                       
** Dissertation-related Research Publications
     [1] Alkhaldi, E. & Salari, E. (2019) Genetically Optimized
     Heterogeneous Ensemble for Histological Image
     Classification. International Journal of Science and
     Engineering Investigations (IJSEI), 8(95), 113-118.

     [2] E. Alkhaldi and E. Salari, “Adaptive PSO-Based Ensemble Optimization for Histology
     Image Classification,”. International Journal of Computer Science and Technology (IJCST), vol. 8491, pp. 26–34, 2021.
* Academic Progress and Research Timeline                          :noexport:
** GANTT Chart
#+CAPTION: 2017-2018 GANNT Chart
[[~/Dropbox/proposal/images/GANNT1.png]]

#+CAPTION: 2019-2020 GANNT Chart
[[~/Dropbox/proposal/images/GANNT2.png]]

#+CAPTION: 2021 GANNT Chart
[[~/Dropbox/proposal/images/GANNT3.png]]

* Questions?
  Questions?
  
\begin{frame}[allowframebreaks]
        \frametitle{References}
        \bibliographystyle{ieeetr}
        \bibliography{/home/alkhaldieid/work/res/cited_lib.bib}
\end{frame}
