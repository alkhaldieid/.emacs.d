% Created 2022-04-27 Wed 06:37
% Intended LaTeX compiler: pdflatex
\documentclass[presentation]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{algpseudocode}
\usetheme{Antibes}
\author{Eid Alkhaldi}
\date{\today}
\title{Ensemble Optimization for Histological Image Classification}
\hypersetup{
 pdfauthor={Eid Alkhaldi},
 pdftitle={Ensemble Optimization for Histological Image Classification},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 28.1 (Org mode 9.5.2)}, 
 pdflang={English}}
\begin{document}

\maketitle

\section*{Motivation and Challenges}
\label{sec:org6dc173c}
\begin{frame}[label={sec:orgae9ca6f}]{Importance of Breast Histological image classification}
\begin{itemize}
\item Breast cancer is the most common type of cancer amongst women worldwide
\item leading cause of cancer-related deaths
\item More than 10\% of women in the U.S. will be diagnosed with breast cancer
\item most prevalent amongst women in the US
\begin{itemize}
\item 252,000 invasive carcinoma new cases
\item 63,000 In Situ carcinoma
\item 40,000 deaths
\end{itemize}
\alert{\alert{\alert{\uline{Yearly!}}}} \cite{Aresta2019}
\item Rare in men
\end{itemize}
\end{frame}
\begin{frame}[label={sec:orgf09ba68}]{Early Diagnosis}
\alert{Early diagnosis} is important for:
\begin{itemize}
\item Reduces Disease Progression
\item Reduce Morbidity rate, increase survival
\item Prognosis and treatment plan
\item Better chance of recovery
\item Reducing healthcare costs
\item Self check-ups!
\end{itemize}
\end{frame}
\begin{frame}[label={sec:orgfef0451}]{Importance of Breast Histological image classification}
\begin{itemize}
\item Microscopic analysis of biopsy is the \alert{most accurate} diagnosis method
\item Demands \alert{extensive} work
\item Requires \alert{specialized} expertise (pathologists)
\item \alert{Time consuming} due to \alert{scarcity of pathologists}
\item Expensive to hire
\item Pathologists often \alert{disagree}
\end{itemize}
\end{frame}
\begin{frame}[label={sec:org57c08b0}]{Importance of Breast Histological image classification}
\begin{itemize}
\item An automated intelligent approach is needed to compensate for the time delay of analysis caused by the insufficient number of pathologists
\item The aim of histology image classification methods is to identify abnormalities in the specimens' structures and specify their carcinogenic level
\end{itemize}
\end{frame}
\begin{frame}[label={sec:org8a3688f}]{Challenges}
\begin{itemize}
\item Deep Learning (DL) showed remarkable competence
\begin{itemize}
\item Extract complex features
\item Expands class separability better than conventional ML
\item CNNs are the most reliable DL models
\end{itemize}
\end{itemize}
\begin{itemize}
\item Limitations:
\begin{itemize}
\item CNNs has a huge number of parameters
\item It demands a high quantity of labeled images
\item Curse of Dimensionality
\begin{itemize}
\item The larger the dataset, the more training time is required
\end{itemize}
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}[label={sec:orgb37adb7}]{Challenges}
\begin{itemize}
\item Consequently, end-to-end training of CNNs suffered significantly, due to the scarcity of annotated hematoxylin and eosin (H\&E) stained histology images.
\item End-to-end training is data greedy
\item End-to-end training using an insufficient number of classified examples is prone to overfitting regardless of how the weights were initialized
\end{itemize}
\end{frame}
\begin{frame}[label={sec:org4996cac}]{Challenges}
\begin{itemize}
\item Transfer learning is the reusing of a model trained on a large dataset for a different task
\item The general layers learns general features
\item Training only the task specific layers is very efficient
\end{itemize}
\end{frame}
\begin{frame}[label={sec:org34dd08e}]{Challenges}
\begin{itemize}
\item Transfer Learning proved to be a more reliable alternative to  end-to-end training
\begin{itemize}
\item Improves accuracy and robustness
\item Reduces the training time significantly, particularly for small datasets
\end{itemize}
\item Limitations:
\begin{itemize}
\item Could result in overfitting, mainly when training the general layers
\item It lacks the ability to separate task-specific layers from general ones
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}[label={sec:orgd3c4e33}]{Challenges}
\begin{itemize}
\item Ensembles are a vertical or cascaded combination of the features or the predictions of accurate models to produce a better performing model
\begin{itemize}
\item Ensembling a number of models proved to be very effective and more robust alternative
\item For ensembles to perform better than its individual model, the models have to be heterogeneous
\begin{itemize}
\item Why?
\begin{itemize}
\item To avoid making the same misclassifications
\end{itemize}
\end{itemize}
\end{itemize}
\end{itemize}
\end{frame}
\section*{Proposed Solutions}
\label{sec:orgd1d48c1}
\begin{frame}[label={sec:orgbdb5ce4}]{Proposed Solutions}
\begin{itemize}
\item Genetically Optimized Heterogeneous Ensemble for Histolgy Image Classification
\item Adaptive PSO-Based Ensemble Optimization for Histology Image Classification
\item Ensemble Optimization Using Cartesian Genetic Programming
\item Ensemble Optimization Using Colonal Selection Algorithm
\end{itemize}
\end{frame}
\section*{Genetically Optimized Heterogeneous Ensemble for Histolgy Image Classification}
\label{sec:org9d0f2cc}
\begin{frame}[label={sec:org5a53da2}]{Meta-training}
The first proposed methods is composed of two phases:
\begin{itemize}
\item Meta-training
\item Ensemble training
\end{itemize}
The meta-training is composed of three stages:
\begin{itemize}
\item Preprocessing
\item GA block
\item Transfer Learning and one cycle policy
\end{itemize}
\end{frame}
\begin{frame}[label={sec:orge4f5d61}]{Preprocessing}
\begin{itemize}
\item The aim of preprocessing is to augment the dataset
\item The histological datasets are usually small due to the lack of data
\item Affine Transformation applied
\begin{itemize}
\item random cropping
\item rotation
\item width/height shifting
\item vertical/horizontal flipping
\end{itemize}
\item Resizeing and cropping to a lower resoluiton
\item Normalization
\end{itemize}
\end{frame}
\begin{frame}[label={sec:orge85fbd5}]{Introduction}
\begin{itemize}
\item Transfer Learning is the process of re-utilization of models' weights that were trained on benchmark datasets such as ImageNet
\begin{itemize}
\item More reliable, robust and accurate than end-to-end
\item More time-efficient
\end{itemize}
\item The weights of a model that was trained end-to-end is used as the initialization for the training of images in a different domain
\item For the new domain, the parameters of the domain-specific layers only are trainable
\item All other weights are frozen
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org9ce2428}]{Transfer Learning Last layers}
\begin{center}
\begin{tabular}{llr}
Layer & Shape & trainable params.\\
\hline
global\textsubscript{average}\textsubscript{pooling2d} & (None, 2048) & 0\\
gaussian\textsubscript{dropout} & (None, 2048) & 0\\
dense & (None, 4) & 8196\\
\end{tabular}
\end{center}
\end{frame}
\begin{frame}[label={sec:org8a81a94}]{Gaussian Dropout}
\begin{itemize}
\item Why?
\item restricts the classifier from learning the irrelevant particularities of an image
\begin{itemize}
\item standard dropout Blocking several units from firing during the feedforward and the backpropagation steps of training
\item Gaussian Dropout is a combination of dropout at percentage = \(rate\) and Gaussian noise addition with \(mean\) = 1 and std described in \ref{eq:org16a654b}.
\item to avoid weight scaling during testing
\end{itemize}
\end{itemize}
\begin{equation} \label{gaus} \sigma _(gaussian)= \sqrt{rate/(1 - rate)}
\label{eq:org16a654b}
  \end{equation} 
Where \emph{rate} denotes the user control parameter
and \(\sigma _(gaussian)\) refers the dropout Gaussian std
\end{frame}
\begin{frame}[label={sec:orgd4fb21a}]{Learning Rate Scheduler}
Why LRS?
\begin{itemize}
\item Aims to minimize the cost function
\item Avoids saddle points in the search spaces which are common in high dimensional problems when LR is too small
\item However, over-increasing the LR will cause fluctuations with high spikes as shown in figure \ref{fig:org905b819}
\end{itemize}
\end{frame}
\begin{frame}[label={sec:org10d8c9f}]{One Cycle Policy}
Triangular Cyclical Learning Rates and illustrated by smith in figure \ref{fig:orgf749f22}. 
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/alkhaldieid/repos/writings/proposal/paper/second_draft/tri.png}
\caption{\label{fig:orgf749f22}Smith's Triangular Cyclical Learning Rates Method.}
\end{figure}
\end{frame}
\begin{frame}[label={sec:orgfd67220}]{Cyclical Learning Rates}
\begin{figure}[htbp]

\includegraphics[width=12cm,height=4cm]{/home/alkhaldieid/Dropbox/finalV/diagrams/first/cyc.png}
\caption{\label{fig:org905b819}High spikes of validiation loss and accuracy curves over exponential cyclical learning rate in a single epoch in ResNet50 with freeze layer = 141 and a batch size = 32. The learning rate with the lowest validation loss at lr = 0.05 is chosen as the base lr for the full training.}
\end{figure}
\end{frame}

\begin{frame}[label={sec:orgb4476eb}]{GA for optimum freeze layer selection}
\begin{itemize}
\item The objective of implementing the genetic algorithm is to evolve the solutions to the fittest task-specific layer
\item GA is an optimization method inspired by simulating the natural selection through crossover and mutation \cite{Singh2018}
\item The possible layers can't be batch normalizaiton layers
\end{itemize}

\begin{itemize}
\item Each possible layer is encoded by a binary chromosome
\item After calculating the initial population fitnesses, the fittest layers are chosen for crossover at a predefined crossover probability Pc to produce better offsprings in the next generation.
\item All layers, except BN, are frozen upto the decoded layer
\end{itemize}
\end{frame}
\begin{frame}[label={sec:orgcc775e4}]{GA for optimum freeze layer selection}
\begin{itemize}
\item cyclical LR is implemented to pick the LR that is most appropriate for the decoded freeze-layer
\item The model is then trained on lower resolution images for 10 epochs
\item The cross-entropy loss is then calculated on the validation dataset
\item The fittest model is then trained thoroughly with higher resultion images
\end{itemize}
\end{frame}
\begin{frame}[label={sec:org56c5f2a}]{GA Steps}
\begin{enumerate}
\item Population and empty best solution chromosome and cost Initialization
\item Calculate cost for each chromosome and \alert{cache}
\item update best solution and best cost
\item Parent Selection by Roulette Wheel
\item Crossover \& Mutate offsprings
\item Combine Off-springs with selected parents
\item sort costs with ascending order
\item Eliminate chromosomes with highest cost
\item Criterion met? --> terminate, else: go to 3
\end{enumerate}
\end{frame}
\begin{frame}[label={sec:orgc1084a1}]{Objective Function}
\begin{enumerate}
\item Check if chromosome is cached
\item decode chromosome
\item freeze layers up to the decoded chromosome
\item pick LR based on one cycle policy \cite{smith2018}
\item train for a few epochs on low resolution images
\item Cost = categorical cross-entropy over the validation set
\end{enumerate}
\end{frame}
\begin{frame}[label={sec:org985fd4e}]{GA for optimum freeze layer selection}
\begin{figure}[htbp]

\includegraphics[width=12cm,height=5.5cm]{/home/alkhaldieid/Dropbox/finalV/diagrams/first/xcepNoGA.png}
\caption{Xception network without GA block converges with a small number of epochs}
\end{figure} 
\end{frame}
\begin{frame}[label={sec:org3fb388b}]{GA for optimum freeze layer selection}
\begin{figure}[htbp]
\centering
\includegraphics[width=12cm,height=5.5cm]{/home/alkhaldieid/Dropbox/finalV/diagrams/first/xcepGA.png}
\caption{Xception network with GA block converges with a small number of epochs}
\end{figure} 
\end{frame}
\begin{frame}[label={sec:org1d984bd}]{Full Training of Ensemble}
\begin{itemize}
\item 4 of the best performing models heterogeneous models are chosen based on their accuracy and misclassifications diversity
\item The predictions are stacked horizontally by a flat layer and densely connected to the labels
\item the weights are then trained on the holdout dataset with a much higher resolution
\end{itemize}
\end{frame}

\section*{Experimental Setup}
\label{sec:org1f92d1a}
\begin{frame}[label={sec:orgbfb455e}]{Dataset}
The images are classified according to their predominant cancer type into 4 classes:

\begin{table}[htbp]
\caption{\label{tab:org4766208}BACH: ICIAR 2018 Grand Challenge on \alert{BreAst Cancer Histology} images}
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{2}{|l|}{Non-carcinoma} & \multicolumn{2}{l|}{Carcinoma} \\
\hline
Normal & Benign & In Situ & Invasive \\
\hline
100 & 100 & 100 & 100 \\
\hline
\end{tabular}
\end{table}
\end{frame}
\begin{frame}[label={sec:org3b5e5dd}]{For training}
\begin{table}[htbp]
\caption{\label{tab:org13100fd}The Distribution of the ICIAR Dataset}
\centering
\begin{tabular}{lrrrr}
Type & Normal & Benign & InSitu & Invasive\\
\hline
Train & 90 & 90 & 90 & 90\\
validation & 5 & 5 & 5 & 5\\
Held-out test & 5 & 5 & 5 & 5\\
\end{tabular}
\end{table}
Test = 100 unlabeled
\end{frame}
\begin{frame}[label={sec:orgc1a6ea4}]{Models The constructed the ensemble}
\begin{table}[htbp]
\caption{\label{tab:orga78ac9d}The models used to construct the ensemble.}
\centering
\begin{tabular}{rllr}
\# & Model Name & namber of parameters & number of layers\\
1. & DenseNet201 & 20.2M & 402\\
2. & ResNet50 & 25.6M & 107\\
3. & InceptionResnetv2 & 55.9M & 449\\
4. & Xception & 22.9M & 81\\
\end{tabular}
\end{table}
\end{frame}
\begin{frame}[label={sec:orgef15625}]{Evaluation Metrics}
\begin{itemize}
\item Accuracy:  the ratio of correct predictions to the total number of samples \cite{Duchesnay2019}
\end{itemize}
\begin{equation}
  acc =  \frac{TP +  TN}{\#samples}
\end{equation}
\begin{itemize}
\item Precision: the rate of correct class predictions to the total number of
samples belonging to that class  \cite{Duchesnay2019}
\end{itemize}

\begin{equation}
\label{eq:org2156e0c}
Precision = \frac{TP}{TP+ FN}
\end{equation}

\begin{itemize}
\item Recall: the true positive rate \cite{Duchesnay2019}
\end{itemize}

\begin{equation}
\label{eq:orgee16965}
Recall = \frac{TP}{TP+ FN}
\end{equation}
\end{frame}

\begin{frame}[label={sec:orgf5bd357}]{Evaluation Metrics}
\begin{itemize}
\item F1 score which is defined by the following equation:
\end{itemize}
\begin{equation}
F1 score = 2 \times \frac{Precision \times Recall}{(Precision + Recall)}
\end{equation}
\begin{itemize}
\item log-loss: the cross-entropy loss, which is the negative log-likelihood of the class labels given predictions \cite{Duchesnay2019}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org68dd226}]{Results}
Table \ref{tab:orgd822c98}
\begin{table}[htbp]
\caption{\label{tab:orgd822c98}Maximum Prediction voting classification report}
\centering
\begin{tabular}{lrrr}
\{Class. Report\} & precision & recall & f1-score\\
Benign & 1.00 & 0.37 & 0.54\\
InSitu & 0.60 & 1.00 & 0.75\\
Invasive & 1.00 & 0.93 & 0.97\\
Normal & 0.97 & 1.00 & 0.98\\
\end{tabular}
\end{table}
\end{frame}

\begin{frame}[label={sec:org12b6644}]{Results}
\begin{table}[htbp]
\caption{\label{tab:org1f817b8}Average Voting Classification Report}
\centering
\begin{tabular}{lrrr}
\{Class. Report\} & precision & recall & f1-score\\
Benign & 1.00 & 0.70 & 0.82\\
InSitu & 0.97 & 1.00 & 0.98\\
Invasive & 1.00 & 1.00 & 1.00\\
Normal & 0.79 & 1.00 & 0.88\\
\end{tabular}
\end{table}
\end{frame}

\begin{frame}[label={sec:org993ec24}]{Results}
\begin{table}[htbp]
\caption{\label{tab:org7526ed3}Proposed Voting Classification Report}
\centering
\begin{tabular}{lrrr}
\{Class. Report\} & precision & recall & f1-score\\
Benign & 1.00 & 0.73 & 0.85\\
In Situ & 0.97 & 1.00 & 0.98\\
Invasive & 1.00 & 1.00 & 1.00\\
Normal & 0.81 & 1.00 & 0.90\\
\end{tabular}
\end{table}
\end{frame}

\begin{frame}[label={sec:org023065a}]{Results}
\begin{table}[htbp]
\caption{\label{tab:org683471f}Evaluation of The Proposed Method Compared to Maximum and Average voting}
\centering
\begin{tabular}{lrrr}
Eval. /method & Proposed & Max. & Avg.\\
Probabilities auc roc & 0.9994 & 1.0 & 0.9934\\
Labels auc roc & 0.95556 & 0.8833 & 0.95\\
Labels accuracy & 0.933 & 0.825 & 0.925\\
Probabilities precision & 0.9985 & 0.94 & 0.988\\
Labels precision & 0.8946 & 0.760 & 0.883\\
Labels log loss & 2.302 & 6.0442 & 2.590\\
ICIAR acc. & \textbf{88\%} & 85\% & 87\%\\
\end{tabular}
\end{table}
\end{frame}
\section*{Adaptive PSO-Based Ensemble Optimization for Histology Image Classification}
\label{sec:org7a396d0}
\begin{frame}[label={sec:orge2b41c5}]{Overview}
\begin{itemize}
\item The main objective of Ensemble optimization is finding a robust composite of several classifiers in order to cultivate a single larger and more accurate model
\item The search space of all possible configurations of a stacked ensemble model is massive
\item brute-forcing and grid-search are impractical
\item manual selection of hyperparameter requires experience and doesn't necessarily produce ideal results
\end{itemize}
\end{frame}
\begin{frame}[label={sec:orga58172e}]{Conventional Ensemble Methods Disadvantages}
Majority and Maximum voting
\begin{itemize}
\item assumes the optimum weights of the classifiers to be equal
\item This assumption undermines the complexity of medical applications
\end{itemize}
But, in histological image are classified into their predominant cancer type!
This leads to classification more biased toward abnormal labels!
\end{frame}

\begin{frame}[label={sec:org0c1555a}]{Meta Training}
\begin{itemize}
\item Transfer Learning Last layers
\end{itemize}
\begin{center}
\begin{tabular}{llr}
Layer & Shape & trainable params.\\
\hline
global\textsubscript{average}\textsubscript{pooling2d} & (None, 2048) & 0\\
gaussian\textsubscript{dropout} & (None, 2048) & 0\\
dense & (None, 4) & 8196\\
\end{tabular}
\end{center}
\begin{itemize}
\item RandAugment for data augmentation
\begin{itemize}
\item why Rand-Augment?
\end{itemize}
\item One Policy Strategy to find proper initial LR
\end{itemize}
\end{frame}
\begin{frame}[label={sec:org4f01840}]{PSO}
\begin{itemize}
\item Bio-inspired Computational algorithms proved to be very efficient in solving optimization problems
\item Particle Swarm Optimization (PSO) is one of the most popular techniques used in NP hard problems and optimization methods for non-convex search-space
\item Swarm-based optimization techniques were developed to mimic the social behaviour of groups of animals that are navigating their way cooperatively to find food resources
\item PSO ws first proposed by Eberhart and Kennedy in 1995 as a paradigm for Neural Networks weights optimization
\end{itemize}
\end{frame}
\begin{frame}[label={sec:org3d83991}]{PSO}
\begin{itemize}
\item Its aims is to find the global minimum of objective functions with complex search space
\item PSO starts with initialization of a generation that has a specified number of candidate solutions called particles
\item In each epoch, particles move in the defined finite search space of the problem based on how well the whole swarm of birds is performing
\item The swarm share the best position achieved by the whole swarm, which is the global best position and fitness.
\end{itemize}
\end{frame}
\begin{frame}[label={sec:orgc89a785}]{PSO}
\begin{itemize}
\item PSO history
\item Swarm-based optimization
\item Mimics social behaviour of a group animals
\item Eberhart and Kennedy \cite{Eberhart1995}
\end{itemize}
\end{frame}
\begin{frame}[label={sec:org5d941e7}]{NN encoding for PSO}
\begin{itemize}
\item each particle of the PSO is composed of the weights and the biases of the Neural Network that fuse the predictions of the ensemble models.
\item The feed-forward NN is constructed as follows:
\begin{enumerate}
\item The input layer is a [1x16] vector which represents the output predictions of the individual models.
\item The second layer is densely connected to the input layer and to \(b1\) bias term.
\item The \(W1\) vector holds the [1x128] weights that connects the input layer to the hidden layer.
\item \(W2\) contains the weights connecting the hidden layer to the output.
\item The output of the hidden layer is the output \(a1\) of the hyperbolic tan applied on the logits  vector defined in equation \ref{eq:orga677626}.
\end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org014c568}]{NN encoding for PSO}
\begin{equation}
\label{eq:orga677626}
 a1 = tanh(X \cdot W1 + b1)
\end{equation}

The output of the last layer is the final predictions vector whose loss is optimized over the validation set and is defined by equation \ref{eq:org2ab7472}. 

\begin{equation}
\label{eq:org2ab7472}
 y = softmax(a1 \cdot W2 + b2)
\end{equation}

Where softmax is an activation function that maps the logits into a probability distribution as shown in equation \ref{eq:org9cb3b0f} \cite{Nwankpa2018}. 

\begin{equation}
\label{eq:org9cb3b0f}
 f(x_i) = \frac{\exp(x_i)}{\sum_j(exp(x_j))}
\end{equation}
\end{frame}

\begin{frame}[label={sec:org98bc87a}]{PSO particle}
The proposed NN is shown in Figure \ref{fig:org7cacc44}.
\begin{itemize}
\item Each particle \(P_i\) in the PSO swarm represents the weights and the biases of the NN shown in \ref{fig:org7cacc44}.
\(P_i\) = [W1,b1,W2,b2] which is a [1x172].

\item The categorical cross entropy is the objective function that the PSO is minimizing and is defined in \ref{eq:org9c33e7f} \cite{patterson2017}.
\end{itemize}

\begin{equation}
\label{eq:org9c33e7f}
  Loss = - \sum_{i=1}^N \sum_{j=1}^C y_{i,j} \times \log \hat{y_{i,j}}
\end{equation}
\end{frame}
\begin{frame}[label={sec:org2c6c184}]{PSO particle}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.3\textwidth]{/home/alkhaldieid/Dropbox/proposal/presentation/lab/psoNN.png}
\caption{\label{fig:org7cacc44}The proposed NN that combine the predictions of the four models to the final prediction}
\end{figure}
\end{frame}



\begin{frame}[label={sec:org9052aa9}]{PSO Velocity and Position update}
The velocity and the position of the particle are each updated based on the equation \ref{eq:org63af547} and \ref{eq:org3ce9e68}. 
\begin{equation}
\label{eq:org63af547}
    V^{t+1}_{i}=w V^{t}_{i}+c_1 r_1\left(X_{pbest_{i}}-X^t_{i}\right) +c_2r_2\left(X_{gbest}-X^t_i\right)
\end{equation}

\begin{equation}
\label{eq:org3ce9e68}
   x_i^{t+1}=x_i^{t}+V_i^{t+1}
\end{equation}
\end{frame}

\begin{frame}[label={sec:org07488e3}]{PSO Velocity and Position update}
\begin{itemize}
\item Where \(V^{t+1}_{i}\) is the updated velocity for particle i,
\item \(V_i^{t}\) is its current velocity,
\item \(w\) is the inertia weight,
\item \(c1\) and \(c2\) are acceleration constants referred to as the cognitive learning rate,
\item \(X_{gbest}\) and \(X_{pbest}_i\) are the global and the personal best positions respectively, and
\item \(r1\), \(r2\) are random numbers evenly distributed \(\in\) [0,1] \cite{He2016}.
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org2ed58a1}]{PSO advantages}
\begin{itemize}
\item Robustness
\item Ease of implementation
\item Remarkable time efficiency
\end{itemize}
\end{frame}
\begin{frame}[label={sec:org7b26456}]{Standart PSO limitations}
\begin{itemize}
\item PSO is highly reliant on the proper tuning of its learning hyper-parameters
It is especially sensitive to:
\begin{itemize}
\item inertia \(w\) (exploitation vs exploration trade-off)
\item accelerate constants \(c1\) and \(c2\) (cognitive and social params)
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}[label={sec:org8392970}]{How \(w\) controls the convergence of PSO}
\begin{block}{Exploration vs. Exploitation}
\begin{itemize}
\item regulates degree of the influence that the earlier velocity has on the updated velocity
\item Therefore, it controls the speed at which a particle moves in the search space.
\item The larger the inertia weight is, the faster the particle moves and the less likely it will be to get trapped on a local minimum
\item However it could escape the local minimum if it is too large
\end{itemize}
\end{block}
\end{frame}
\begin{frame}[label={sec:org7503856}]{Mamdani FIS for adaptive PSO}
\begin{itemize}
\item Fuzzy Inference Systems (FIS) were designed to map fuzzy inputs that belong to fuzzy sets with varying degrees to fuzzy outputs
\item FIS has been successfully applied to the adaptation of parameters for numerous algorithms
\item FIS deals with parameters that belong to classes that are not clearly defined
\item A fuzzy set is a set whose elements have a degree of association to one or more fuzzy classes
\end{itemize}
\end{frame}
\section*{Mamdani FIS steps}
\label{sec:orgac64469}
\begin{frame}[label={sec:org17394d0}]{fuzzification of the inputs and the outputs}
\begin{equation}
\begin{gathered}[b]
    \mu(x,a,b,c) = \max(\min(\frac{x-a}{b-a},\frac{c-x}{c-b}),0)
\end{gathered}
\end{equation}
\end{frame}
\begin{frame}[label={sec:org3215fd4}]{IF-THEN Rule evaluation}
\begin{table}[htbp]
\caption{\label{tab:orgfdb57b1}The Mamdani Fuzzy Rules}
\centering
\begin{tabular}{rll}
Rule number & antecedents & consequent\\
\hline
1 & d1 = high, d2 = high,   NCF = high & w = high\\
2 & d1 = low , d2 = medium, NCF = medium & w = high\\
3 & d1 = low, d2 = low,   NCF = medium & w = high\\
4 & d1 = low, d2 = low,   NCF = high & w = high\\
5 & d1 = low, d2 = low,   NCF = low & w = low\\
\end{tabular}
\end{table}
\end{frame}
\begin{frame}[label={sec:org44a0bed}]{Rules intuition}
\begin{itemize}
\item When all the metrics are low, the optimal solution is close.
Therefore, the value of \(w\) is set to low in order to increase the exploitation to refine the local search.
\item Otherwise, the region of interest is not near yet.
As a result, the exploration is increased by giving a high value for the inertia \(w\).
\end{itemize}
Figure \ref{fig:orgf5eae8f} shows the fuzzy membership functions of the antecedents and the consequents rules of the FIS. 
\end{frame}
\begin{frame}[label={sec:org68474c3}]{Rule Evaluation}
\begin{figure}[htbp]
\centering
\includegraphics[width=12cm,height=5.5cm]{/home/alkhaldieid/repos/writings/proposal/paper/second_draft/fuzzy.png}
\caption{\label{fig:orgf5eae8f}The Mamdani FIS membership functions for the fuzzification of the PSO Inertia Weight}
\end{figure}
\end{frame}
\begin{frame}[label={sec:orgb80422a}]{Aggregation of the rule output}
The Aggregate output of the rule evaluation is a scaled and cropped version of the fuzzy inputs 
\end{frame}

\begin{frame}[label={sec:org4a927d3}]{Defuzzification}
The crisp output of the FIS is calculated using equation \ref{eq:orgd6d4310}.
\begin{equation}
\label{eq:orgd6d4310}
   Center\; of\; Gravity (COG) = \frac{\sum\limits_{x=a}^{b}\mu_A(x)x}{\sum\limits_{x=a}^{b}\mu_A(x)}
\end{equation}
\end{frame}
\begin{frame}[label={sec:org55762a2}]{Mamdani FIS for adaptive PSO}
\begin{figure}[htbp]
\centering
\includegraphics[width=12cm,height=5.5cm]{/home/alkhaldieid/Dropbox/proposal/presentation/lab/pso_alg.png}
\caption{\label{fig:org64a2700}Adaptive PSO Algorithm}
\end{figure}
\end{frame}
\begin{frame}[label={sec:orgf2fd5c6}]{Models that constructed the ensmble}
\begin{table}[htbp]
\caption{\label{tab:org1a8cb87}The models used to construct the ensemble.}
\centering
\begin{tabular}{rllr}
\# & Model Name & number of parameters & number of layers\\
1. & InceptionV3 & 23.9M & 189\\
2. & NasNetLarge & 5.3M & 389\\
3. & DenseNet201 & 20.2M & 402\\
4. & ResNet152V2 & 60.4M & 107\\
\end{tabular}
\end{table}
\end{frame}
\begin{frame}[label={sec:orgb30d155}]{Results}
Table \ref{tab:orgf71fb2a} show that our approach outperformed the weighted average method by 2\% accuracy. 

\begin{table}[H]
\caption{\label{tab:orgc915e8e}Maximum Voting Classification Report}
\centering
\begin{tabular}{lrrr}
\{Class. Report\} & precision & recall & F1-score\\
\hline
Benign & 1.00 & 0.37 & 0.54\\
InSitu & 0.60 & 1.00 & 0.75\\
Invasive & 1.00 & 0.93 & 0.97\\
Normal & 0.97 & 1.00 & 0.98\\
\end{tabular}
\end{table}
\end{frame}

\begin{frame}[label={sec:orge0c0cdd}]{Results}
\begin{table}[H]
\caption{\label{tab:orged8c591}Proposed Voting Classification Report}
\centering
\begin{tabular}{lrrr}
\{Class. Report\} & precision & recall & F1-score\\
\hline
Benign & 1.00 & 0.87 & 0.95\\
In Situ & 0.98 & 0.99 & 0.98\\
Invasive & 0.99 & 0.99 & 0.99\\
Normal & 0.92 & 0.94 & 0.91\\
\end{tabular}
\end{table}
\end{frame}

\begin{frame}[label={sec:orgbe4e87b}]{Results}
\begin{table}[H]
\caption{\label{tab:orgf71fb2a}Evaluation of The Proposed Method Compared to Average voting}
\centering
\begin{tabular}{lrr}
Eval. metric & Proposed & Avg.\\
\hline
P. AUC ROC & 0.9983 & 0.9934\\
L. AUC ROC & 0.966 & 0.95\\
L. accuracy & 0.951 & 0.925\\
P. precision & 0.99 & 0.988\\
L. precision & 0.9033 & 0.883\\
L. log loss & 2.108 & 2.590\\
ICIAR acc. & 89\% & 87\%\\
\end{tabular}
\end{table}
Where \emph{P. AUC ROC} stands for the area under the ROC curve for the output probabilities, while \emph{L. AUC ROC} represents the area under the ROC curve for the one hot encoding for the labels produced by choosing 1 for the class with highest probability and 0 otherwise. 
\end{frame}

\section*{Ensemble Optimization Using Cartesian Genetic Programming}
\label{sec:org1c2efd7}
\begin{frame}[label={sec:org3180103}]{Evolutionary Algorithms in ensembles are generally used for:}
\begin{enumerate}
\item hyperparameter optimization
\item topology (design) optimization
\end{enumerate}
\end{frame}
\begin{frame}[label={sec:org2a83b71}]{Standard CGP}
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/alkhaldieid/Dropbox/third/cgp.png}
\caption{\label{fig:org3ad3d10}The standard CGP representation of the Neural Network and the encoded gene \cite{Miller2011}}
\end{figure}
\end{frame}

\begin{frame}[label={sec:org729f498}]{The CGP method focuses on determining the most optimum architecture of the stacked ensemble}
\begin{itemize}
\item Why CGP?
\begin{itemize}
\item because it is more flexible than GA
\item reduces software bloat
\end{itemize}
\item A solution can be represented in a Cartesian form as shown in Figure \ref{fig:orgf4d1b39}
\end{itemize}
\end{frame}
\begin{frame}[label={sec:orge0c08d9}]{dCGPANN Variant}
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/alkhaldieid/Dropbox/third/dcgp.png}
\caption{\label{fig:orgb4ef31f}The dCGP representation of the Neural Network and the additions of weights and biases to the encoded gene \cite{Martens2019}}
\end{figure}
\end{frame}

\begin{frame}[label={sec:org7a6213f}]{dCGPANN used algorithm}
\begin{center}
\includegraphics[width=.9\linewidth]{/home/alkhaldieid/Dropbox/proposal/presentation/lab/algo.png}
\label{org0039ee1}
\end{center}
\end{frame}
\begin{frame}[label={sec:org5b95a6e}]{Cartesian Genetic Programming}
\begin{figure}[htbp]
\centering
\includegraphics[width=12cm,height=5.5cm]{/home/alkhaldieid/Dropbox/third/ensemble_gene3.png}
\caption{\label{fig:orgf4d1b39}A randomly generated ensemble topology by CGP}
\end{figure}
\end{frame}
\begin{frame}[label={sec:orgbc0d185}]{The proposed method is composed of two stages.}
\begin{block}{Stage 1}
\begin{enumerate}
\item fine-tune and train end-to-end multiple heterogeneous
\item select the best-performing models
\item The test dataset patches are then augmented to 5000 images.
\end{enumerate}
\end{block}
\begin{block}{Stage 2}
\begin{enumerate}
\item The classifiers' predictions of the 5000 images are used to train the stacked ensemble using dCGPANN.
\item the inputs of the dCGPANN is [8x1] vector which is the output of the individual models
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}[label={sec:org8af09b2}]{Cartesian Genetic Programming}
\begin{itemize}
\item The predictions of the 5000 images are used to train the stacked ensemble using CGP
\item CGP inputs are a 16x1 vector
\item The solutions are then evolved using crossover and mutation operators based on predefined parameters.
\end{itemize}
\end{frame}
\section*{Ensemble Optimization Using CSA}
\label{sec:org9dbd767}
\begin{frame}[label={sec:org2d391d6}]{Immunecomputing}
\begin{itemize}
\item Colonal Selection Algorithm is bio-inspired by the immune systems in mammals
\item Immune systems has the capacity to identify harmful infected cells (antigens)
\item In response, the immune system produces antibodies to fight antigens
\item Antibodies are then cloned based on the degree of antibodies-antigens recognition score called Affinity Score
\end{itemize}
:Effort:   3
\end{frame}
\begin{frame}[label={sec:org6adbe80}]{Parameters to be optimized}
\begin{itemize}
\item The antibodies with the highest Affinity scores are used as the antigens for the second epoch
\item The new antigens are then mutated
\item The procedure is repeated for a predefined number of iterations or until stop criteria are met
\item the CSA is going to be used to optimize the hyperparameters of the ensemble training
\end{itemize}
\end{frame}
\begin{frame}[label={sec:orgfd506ef}]{Parameters to be optimized}
\begin{itemize}
\item each antigen or antibody contains the hyperparameters of the ensmble such as:
\begin{itemize}
\item number of hidden layers
\item number of Convolutional layers
\item number of epochs
\item batch size
\item Type of optimizers
\end{itemize}
\end{itemize}
\end{frame}
\section*{Experimental Setup}
\label{sec:org692a696}
\begin{frame}[label={sec:orga98b9be}]{Evaluation Metrics}
\label{org517fef4}

\begin{itemize}
\item Accuracy:  the rate of correct predictions to the total number of samples \cite{Duchesnay2019,Javeed2019}.
\end{itemize}
\begin{equation}
  acc = \sum TP + \sum TN/\sum \#samples
\end{equation}
\begin{itemize}
\item Precision: the rate of correct class predictions to the total number of
samples belonging to that class  [[\cite{Duchesnay2019,Javeed2019}.
\end{itemize}
\begin{equation}
 Precision = \sum TP + \sum TN
\end{equation}

\begin{itemize}
\item Recall: the true positive rate \cite{Duchesnay2019,Javeed2019}.
\end{itemize}
\begin{equation}
 TPR = \sum TP /  \#positive \  samples
\end{equation}
\begin{itemize}
\item F1 score
\end{itemize}

\begin{equation}
F1 score = 2 \times \frac{Precision \times Recall}{(Precision + Recall)}
\end{equation}
\begin{itemize}
\item log-loss: the cross-entropy loss, which is the negative log-likelihood of the class labels given predictions \cite{Duchesnay2019,Javeed2019}
\item The area under the ROC curve
\end{itemize}
\end{frame}
\begin{frame}[label={sec:org5311ebf}]{Dataset 1 BACH: ICIAR 2018 Grand Challenge on \alert{BreAst Cancer Histology} images}
\begin{itemize}
\item ICIAR 2018 dataset
\item The histology patches dataset consist of 400 labeled training images
\item 2048 x 1536 pixels resolution
\item cancerous classes of the images is uniform distribution, as shown in Table \ref{tab:orgb2226b5}
\item Highly experienced medical specialists classified the images
\item Classes are shown in Table \ref{tab:org4766208}.
\end{itemize}
\end{frame}
\begin{frame}[label={sec:org00bc289}]{Dataset 1 BACH: ICIAR 2018 Grand Challenge on \alert{BreAst Cancer Histology} images}
\begin{table}[htbp]
\caption{\label{tab:org70c7d54}BACH: ICIAR 2018 Grand Challenge on \alert{BreAst Cancer Histology} images}
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{2}{|l|}{Non-carcinoma} & \multicolumn{2}{l|}{Carcinoma} \\
\hline
Normal & Benign & In Situ & Invasive \\
\hline
100 & 100 & 100 & 100 \\
\hline
\end{tabular}
\end{table}
Unlabeled test images = 100
\end{frame}
\begin{frame}[label={sec:org9cbc102}]{Dataset 1 BACH: ICIAR 2018 Grand Challenge on \alert{BreAst Cancer Histology} images}
\begin{figure}[htbp]
\centering
\includegraphics[width=12cm,height=5cm]{/home/alkhaldieid/Dropbox/second_final/data.png}
\caption{\label{fig:org705d948}Benign, InSitu, Invasive and Normal H\&E stained breast samples (from top left to bottom right)}
\end{figure}
\end{frame}
\begin{frame}[label={sec:org23fa644}]{Dataset 1 BACH: ICIAR 2018 Grand Challenge on \alert{BreAst Cancer Histology} images}
\begin{table}[htbp]
\caption{\label{tab:orgb2226b5}The Distribution of the ICIAR Dataset in our Study.}
\centering
\begin{tabular}{lrrrr}
Type & Normal & Benign & InSitu & Invasive\\
Train & 90 & 90 & 90 & 90\\
validation & 5 & 5 & 5 & 5\\
Held-out test & 5 & 5 & 5 & 5\\
Test & 25 & 25 & 25 & 25\\
\end{tabular}
\end{table}
\end{frame}
\begin{frame}[label={sec:orgd8eb396}]{Dataset 2 IDC}
\begin{itemize}
\item Extracted from 279 patients
\item 277524 non-overlapping 50x50 patches
\item Binary Classes
\end{itemize}
\end{frame}
\begin{frame}[label={sec:orgf139c9c}]{Dataset 2 IDC statistics}
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/alkhaldieid/Dropbox/third/datasethists.png}
\caption{\label{fig:org8d9dd15}Dataset Statistics}
\end{figure}
\end{frame}

\begin{frame}[label={sec:orgc9251d6}]{Dataset 2 IDC Positive Samples}
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/alkhaldieid/Dropbox/third/idcpp.png}
\caption{\label{fig:org89c8b70}IDC positive samples}
\end{figure}
\end{frame}
\begin{frame}[label={sec:org81eaf91}]{Dataset 2 IDC Negative Samples}
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/alkhaldieid/Dropbox/third/idcnp.png}
\caption{\label{fig:orgfd70387}IDC negative samples}
\end{figure}
\end{frame}
\begin{frame}[label={sec:orgf91433b}]{Hardware and Software}
\begin{itemize}
\item The experiments are run on:
\begin{itemize}
\item NVIDIA Geo-Force RTX 2080 GPU
\item AMD Ryzen Threadripper 1950X 16-Core processor
\end{itemize}
\item Using the following open source software
\begin{itemize}
\item Tensorflow
\item Keras
\item Pyton-dcgp
\item DEAP
\item gplearn
\item PySwarm
\item Artix Linux
\item NEAT
\end{itemize}
\end{itemize}
\end{frame}
\section*{Suggested Dissertation Outline}
\label{sec:orgd6e20d1}

\begin{table}[htbp]
\caption{The anticipated dissertation contents part 1}
\centering
\begin{tabular}{rl}
\hline
Chapter & Title\\
\hline
1. & Introduction\\
\hline
2. & Literature Review\\
\hline
3. & Genetically Optimized Heterogeneous Ensemble\\
 & for Histological Image Classification\\
\hline
4. & Adaptive PSO-Based Ensemble Optimization\\
 & for Histology Image Classification\\
\hline
5. & Cartesian Genetic Programming\\
 & for Stacked Ensemble Optimization\\
\hline
6. & Ensemble Training Optimization\\
 & Using Colonal Selection Algorithm\\
\hline
8. & Conclusion and Future Work\\
\hline
\end{tabular}
\end{table}

\begin{frame}[label={sec:org7f05c90}]{Dissertation-related Research Publications}
[1] Alkhaldi, E. \& Salari, E. (2019) Genetically Optimized
Heterogeneous Ensemble for Histological Image
Classification. International Journal of Science and
Engineering Investigations (IJSEI), 8(95), 113-118.

[2] E. Alkhaldi and E. Salari, “Adaptive PSO-Based Ensemble Optimization for Histology
Image Classification,”. International Journal of Computer Science and Technology (IJCST), vol. 8491, pp. 26–34, 2021.
\end{frame}
\section*{Questions?}
\label{sec:orga84fe81}
Questions?

\begin{frame}[allowframebreaks]
        \frametitle{References}
        \bibliographystyle{ieeetr}
        \bibliography{/home/alkhaldieid/work/res/cited_lib.bib}
\end{frame}
\end{document}