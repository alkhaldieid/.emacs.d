% Created 2022-05-10 Tue 09:02
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage[top=0.5in]{geometry}
\usepackage[section]{placeins}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{minted}
{\setlength{\parindent}{0cm}
\usepackage{adjustbox}
\usepackage{float}
\restylefloat{table}
\author{Eid Alkhaldi, Ezzatollah Salari}
\date{\today}
\title{Ensemble Optimization for Invasive Ductal Carcinoma IDC Classification Using Differential Cartesian Genetic Programming}
\hypersetup{
 pdfauthor={Eid Alkhaldi, Ezzatollah Salari},
 pdftitle={Ensemble Optimization for Invasive Ductal Carcinoma IDC Classification Using Differential Cartesian Genetic Programming},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.2 (Org mode 9.4.4)}, 
 pdflang={English}}
\begin{document}

\maketitle
\begin{ABSTRACT}
\textbf{Abstract} \label{org2f30f4c}

\qquad The high cost of acquiring annotated histological slides for breast specimens entails exploiting an ensemble of models appropriately trained on small datasets.  
Histological Image Classification ensembles strive to accurately detect abnormal tissues in the breast samples by determining the correlation between the predictions of its weak learners.  
Nonetheless, the state-of-the-art ensemble methods, such as boosting and bagging, count merely on manipulating the dataset and lack intelligent ensemble decision making.  
Furthermore, the methods mentioned above are short of the diversity of the weak models of the ensemble.  
Likewise, other commonly used voting strategies, such as weighted averaging, are limited to how the classifiers' diversity and accuracy are balanced.
Hence, In this paper, we assemble a Neural Network ensemble that integrates the models trained on small datasets by employing biologically-inspired methods.  
Our procedure is comprised of two stages.
First, we train multiple heterogeneous pre-trained models on the benchmark Breast Histopathology Images for Invasive Ductal Carcinoma (IDC) classification dataset.  
In the second meta-training phase, we utilize the differential Cartesian Genetic Programming (dCGP) to generate a Neural Network that merges the trained models optimally.  
We compared our empirical outcomes with other state-of-the-art techniques.
Our results demonstrate that improvising a Neural Network ensemble using Cartesian Genetic Programming transcended formerly published algorithms on slim datasets.

\textbf{\emph{Keywords}}---  \emph{Cartesian Genetic Programming, hyparameters optimization, Ensemble, CNNs, Histolpathaological Image Classification}
\end{ABSTRACT}
\section{Introduction}
\label{sec:orgc6c8707}
\label{orgd569a5d}

\qquad Histopathology images carcinoma classification for breast specimens, stained with hematoxylin and eosin (H\&E), has been investigated broadly due to the significance of the problem, the shortage of annotated images, and the increased expense of hiring radiologists \cite{Alkhaldi2021,Alkhaldi2019,Cao2018,cruzroa2014}.
Breast cancer is one of the most deadly types of cancer. 
Around four-fifths of the deaths induced by breast cancer are caused by Invasive Ductal Carcinoma (IDC) \cite{cruzroa2014}.  
IDC's diagnosis and prognosis demand examining Whole Slide Images (WSI) of breast biopsies stained with H\&E for visual features improvements.  
The WSI are high-resolution images of the sample, usually divided into smaller sub-images for training  machine learning (ML) classifiers.

\qquad As a consequence of the issue's significance, several techniques and ML pipelines have been suggested in the histopathology image classification literature.  
The earliest methods relied on uniting several useful features to represent the image better. 
These features are handcrafted by observing how the negative IDC patches differ visually from the positive ones \cite{cruzroa2014}.  
Many of these approaches centered around the nuclei features via nuclei segmentation
such as nuclei position, centroid, density, glands segmentation,
Voronoi-based features, Gabor filter, and the HSI color space
\cite{cruzroa2014}.  Most of the handcrafted-based approaches
depended on a composite of numerous characteristics mentioned earlier
that extend the distinguishability of the categories.  Cruz-Roa et
al. \cite{cruzroa2014} presented one of the foremost Convolutional Neural Network (CNN) models to categorize the BHI samples with a CNN \cite{cruzroa2014}.  CNN
revealed an unprecedented advancement over the handcrafted feature
approaches in histopathology image classification
\cite{Alkhaldi2021}.  Nevertheless, the IDC classification of
histopathology images stays challenging as a result of the
insufficiency of data and the increased need of CNNs for labeled
samples.  Accordingly, more evolved techniques are needed to fetch the
most precise predictions, such as transfer learning, domain
adaptation, and ensemble optimization \cite{Alkhaldi2021}.

\qquad Equivalent to blending the handcrafted features, ensemble
learning is concentrated on integrating a set of diverse classifiers
into a more accurate and robust model. The concept is to reduce
variance and biases in the classifiers leading to a more generalized
model that would function better on unseen samples.

\qquad Ensemble optimization has been successfully applied into
numerous applications such as anomaly detection, cyber security, image
classification and a diverse set of applications \cite{You2020}.
Ensemble learning is one of the most successful machine learning
practices \cite{Huang2017a}. Linking a group of complementary
classifiers will generally result in a model that is at least more
accurate than any of its components
\cite{Dietterich2000}. Complementary classifiers have to be both
diverse and accurate enough for the resulting ensemble to be more
robust.

While ensemble learning produces more accurate networks, training
multiple classifiers is time-consuming \cite{Huang2017a}. To
overcome this impediment, Huang et al. \cite{Huang2017a} proposed
saving snapshots of the model during training instead of training
multiple heterogenous models. Huang et al. method reduces the training
time significantly by generating a group of the same model with
different weights.  However, the problem with homogeneous classifiers
is that they are more inclined to overfit the dataset's peculiarities
and are deprived of the diversity of the models.
Additional ensemble learning methods were presented in the literature,
such as Bayesian Averaging, bagging, boosting, vertical voting, and
horizontal stacking
\cite{Xie2013b,Huang2017a,Dietterich2000,Liu2015}.

\qquad Due to the successful implementations of Evolutionary Algorithms
(EA) in optimizing stacked ensembles established in the literature, a
more adaptable EA seems exceptionally advantageous
\cite{Alkhaldi2019,Alkhaldi2021,Liu2015}.  This paper illustrates
the utilization of an evolutionary algorithm to automate selecting the
best Neural Network (NN) configuration with its corresponding weights during
meta-training.  The meta-training shrinks the hyper-parameters search
space considerably, accelerating convergence during the comprehensive
training stage.  It furthermore displays an intuitive measure of
heterogeneity and automatic optimization of the horizontally stacked
prediction vector's wights.

\qquad The existing state-of-the-art transfer learning techniques for
microscopy image classification lack the systematic detachment of the
task-dependent layers from the transferrable ones, resulting in
overfitting when training over a limited quantity of
samples. Moreover, their ensembles have a high number of
hyper-parameters, making it challenging and time-consuming to pick the
optimal ones manually.

\qquad It was established in earlier research that ensemble learning is
indispensable for improving the accuracy of the overall
system. Ensemble learning aspires to enhance the performance of a
meta-classifier which is composed of weak classifiers. The arrangement
of the meta-classifier delivers more acceptable prediction accuracy
than any of the individual models.

\qquad This paper presents a novel approach of utilizing the Cartesian
Genetic Programming Algorithm (CGP) for stacked ensemble optimization.
Our pipeline consists of two primary parts. The first part deals
with training Convolutional Neural Networks with different setups on
the Breast Histopathology Imaging (BHI) dataset.  In the second
portion, we define a cartesian Neural Network with a number of the NN
topology parameters and evolve them using the Differential Cartesian
Genetic Programming. Section \ref{orge9f985c} describes our proposed
methods. Our approach outperforms previously published algorithms as
demonstrated in section \ref{org9f20484}.

\qquad The remaining sections are arranged as follows: Section
\ref{orge9f985c} elaborates on the training of individual
classifiers and the meta-training of the ensemble strategy. Section
\ref{org9f20484} illustrates in detail the experimental setup, the
evaluation metrics, the benchmark dataset and the used computational
resources. Section \ref{org2d1e51d} exhibits the experimentations and
their results compared with diverse state-of-the-art published
methodologies.  Section \ref{org30f40b8} concludes the paper with our
findings, the constraints of our approach, and future research
suggestions.

\section{Proposed Method}
\label{sec:orga796706}
\label{orge9f985c}
\subsection{Overview}
\label{sec:orgc67c84b}
\qquad This section comprehensively describe the differential Cartesian Genetic Programming Artificial Neural Network (dCGPANN) and how it was employed to acquire the optimum topology and utilize the gradients for error back-propagation to update the weights and biases. 
It also examines the implementation details of the individual classifier's training. 
Besides, it depicts the multiple stages of training the ensemble.  

\qquad Evolutionary Algorithms, in general, are used to optimize the parameters of a particular system or to conceive a better architecture \cite{Sekanina}. 
The principal emphasis of the earlier proposed methods was to fine-tune the parameters of the ensemble strategy. 
In numerous algorithms, the numbers of nodes and the connections are fixed, while the ANN weights are evolved. 
Different algorithms only evolve the weights of the ANN using EA. Nonetheless, stochastic gradient descent is far more potent in correcting the weights in the course of training than any other technique. 

\qquad For the ensemble to be more accurate than its composing classifiers, it must be diverse and accurate. 
Nevertheless, diverse and accurate classifiers do not always guarantee a better performing ensemble \cite{You2020}. 
Assembling a diversity-accuracy balanced ensemble is not a straightforward process. 
Thus, a more advanced ensemble technique is needed. 
This chapter concentrates on deciding the most optimum architecture of the stacked ensemble as well as its weights and biases through the standard error back-propagation using the gradient information obtained by the dCGPANN algorithm \cite{Izzo2017,Martens2019,Izzo2020}.

\qquad The proposed method is composed of two stages. The first stage
is to fine-tune and train end-to-end multiple heterogeneous
classifiers on the training datasets to ensure diversity amongst
models. Then we select the best-performing models. The test dataset 
patches are then augmented to 5000 images. The classifiers'
predictions of the 5000 images are used to train the stacked ensemble
using dCGPANN. The CGP inputs are an 8x1 vector of stacked prediction
of the best performing models. The test dataset is used to
evaluate the performance of our method against different cutting-edge
algorithms.

\qquad The genes are then evolved using crossover and mutation
operators based on predefined parameters as explained in \ref{org20fe430}.
The motive of this method is to overcome the limitation that the
previous methods had by using the derivatives of the loss function.
We believe that the reduction of software bloat and the
representational capability of the dCGPANN can produce
state-of-the-art competitive ensemble topologies.
\subsection{The Differential Cartesian Genetic Programming}
\label{sec:orgce80e7d}
\label{org20fe430}

\qquad Genetic Programming is one of the most successfully applied
Evolutionary Algorithm in the optimization domain \cite{Liu2015}. It is
exceptionally suitable for binary classification problems due to its
syntax \cite{Liu2015}. One of the constraints that impede the
performance of GP is program bloat and duplicative calculation of the
same node every time it is needed \cite{Turner2015}. Another
drawback is their lack of ability to evolve topologies. Since its
invention, researchers have developed numerous versions of GP. Several
GP algorithms have been developed such as stacking, tree-based Genetic
Programming, Grammatical Evolution, linear Genetic Programming and
CGP.

\qquad The Cartesian Genetic Programming (CGP) was first formed to evolve
circuit design in the late 90s \cite{Miller2020a,Turner2017,Turner2015a}. The CGP technique
offers more flexibility to realize better ensemble topologies. The most prominent advantage of
CGP over other variants of Genetic Programming is its ability to
express the solution candidates as a cyclic graphs rather than the tree-based variant in the standard GP. CGP is known to reduce redundant
computations \cite{Turner2017}. Unlike standard tree-based GP, a
solution can be represented in a Cartesian form, as shown in Figure
\ref{fig:orgd14a8bd}. Figure \ref{fig:orgd14a8bd} displays a randomly generated ensemble topology
by the Cartesian Genetic Programming.
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/alkhaldieid/Dropbox/third/ensemble_gene3.png}
\caption{\label{fig:orgd14a8bd}A randomly generated ensemble topology by CGP}
\end{figure}

\qquad The capacity of the CGP to represent a candidate solutiona in
two-dimensional gene was revolutionary. Comparable to electrical
circuits for which the CGP was used to evolve, the evolution of an ANN
was investigated. Figure \ref{fig:org61efee0} portrays the standard CGP encoding of
the ANN where Miller et al. used CGP to evolve the ANN architecture
\cite{Miller2011}.

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/alkhaldieid/Dropbox/third/cgp.png}
\caption{\label{fig:org61efee0}The standard CGP representation of the Neural Network and the encoded gene \cite{Miller2011}}
\end{figure}

\qquad The first column from the left of the NN is the input layer, while the last one is the output layer.
Where \(r\) is the number of the rows, \(c\) is the number of the columns,
\(n\) is the number of the features, and \(m\) is the number of the
classes.

The inputs of the Neural Network is the output predictions of the four models that compose the ensemble. 
The output of the Neural Network is the final output probabilities of the whole ensemble.
It is known in the literature that the weights are better updated using the gradient descent
methods which established its supremacy over the past decades
\cite{Izzo2017}. Another deficiency of the original use of CGP is the
absence of biases which is vital to the learning process. Izzo et
al. \cite{Martens2019,Izzo2017,Izzo2020} established a remarkably innovative idea that permits the evolution of the
topology with CGP and updating the weights and biases using Stochastic
Gradient Descent at the same time \cite{Martens2019}. The concept is
to split the gene into two parts. The first portion encodes the
neural network nodes, connections, and activation functions. The second portion of the gene encodes the weights and the biases of the ANN
connections \cite{Martens2019}.  Figure \ref{fig:orgddbcd73} pictures how the modified
CGP incorporated the weights and biases.
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/alkhaldieid/Dropbox/third/dcgp.png}
\caption{\label{fig:orgddbcd73}The dCGP representation of the Neural Network and the additions of weights and biases to the encoded gene \cite{Martens2019}}
\end{figure}

\qquad In the dCGPANN variant, a gene \(X\) representing a candidate solution consists of two portions. \(X_I\) contains the evolutionary part
of the gene which is evolved by the CGP, while \(X_R\) contains the
weights and biases of the ANN as indicated in equation \ref{eq:org47d4957} and \ref{eq:orgeb74eea}
respectively \cite{Martens2019}.

\begin{equation}
\label{eq:org47d4957}
  X_I = [F_0,C_{0,0},C_{0,1},...,C_{0,a},F_1,C_{1,0},...,C_{1,a},..., O_1,...,O_m]
\end{equation}

\begin{equation}
\label{eq:orgeb74eea}
  X_R = [b_0,w_{0,0},w_{0,1},...,w_{0,a_0},b_1,w_{1,0},...,w_{1,a_1},...]
\end{equation}

where \(X_I\) \(\in\) natural number is a vector that encodes the evolutionary
part of the ANN, \(X_R\) \(\in\) real numbers is a vector for the biases \(b\)
and weights \(w\), \(F\) represents functions, \(C\) represent the
connections and \(O\) represent the terminal nodes.

Based on the dCGPANN, the output of each node in the ANN is expressed
in equation \ref{eq:org2c79c4b} \cite{Martens2019}.


\begin{equation}
\label{eq:org2c79c4b}
  N_i =  F_i ( \sum_{j=0}^{a_i} w_{i,j} C_N_{i,j} + b_i )
\end{equation}

Where \(N_i\) is the output of the node \(N\) with id \(i\), \(F_i\) is the
activation function, \(a_i\) is the arity of the node \(i\) which is the
number of connections to that node, \(C_{i,j}\) is the connection
between note \(i\) in the current layer and node \(j\) in the previous
layer and \(b_i\) is the bias associated with the activation function
\(F_i\) in node \(N_i\). The number of connections to each node, arity, is
assumed equal by default in all nodes unless it gets defined by a list
that specifies the number of connections each node should have in a
particular layer. The arity list is a \{1x\(c\)\} vector that
assigns the arity of the nodes in each column \(c\).

\qquad The evolutionary operators \(\mu\)\textsubscript{c} and \(\mu\)\textsubscript{a} are applied on the \(X_I\) part of the gene that expresses the dCGPANN. \(\mu\)\textsubscript{c} and \(\mu\)\textsubscript{a} are fractions to be mutated in active connections gene and active function genese respectively. Each mutant goes through
a predefined number of stochastic gradient descent (SGD) training
epochs during which only \(X_R\) is learned, and \(X_I\) is fixed. Algorithm
\ref{algo} illustrates the steps of how the dCGPANN was operated to
optimize the structure of the ensemble as well as its weights and
biases \(\theta\).
The loss function which the dCGPANN is minimizing is the categorical cross-entropy which is defined in equation \ref{eq:orgb083560} \cite{patterson2017}.

\begin{equation}
\label{eq:orgb083560}
  Loss = - \sum_{i=1}^N \sum_{j=1}^C y_{i,j} \times \log \hat{y_{i,j}}
\end{equation}

Where \(N\) is the number of the input images, \(C\) is the number of classes, \(y_{i,j}\) is the true label of image \(i\) belongs to class \(j\) which is 1 if \(X_i\) \(\in\) \(j\) and 0 otherwise and \(\hat{y_{i,j}}\) is the output probability that image \(i\) \(\in\) class \(j\).


\begin{algorithm}
\caption{dCGPANN}
\label{algo}
\begin{algorithmic}
\REQUIRE $r$ , $c$ , $n$ , $m$ , $l$ , $a$ , $Kernels$, $epochs$, $cycles$
\STATE Randomly Initialize N dCGPANNs
\FOR{dCGPANN in population N}
\STATE Compute the Loss
\ENDFOR
\FOR{j in cycles}
\STATE Select the best dCGPANN of the previous generation
\STATE Delete other dCGPANNs
\FOR{i in population N}
\STATE{ Mutate the best dCGPANN's functions using $\mu_a$}
\STATE{Mutate the best dCGPANN's connections using $\mu_c$}
\FOR{epoch in epochs}
\STATE Run SGD on dCGPANN_i
\ENDFOR
\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
\subsection{Implementation Details}
\label{sec:org5612bc8}
\label{org3e44833}

\qquad This section outlines the implementation details and the handpicked classifiers' and the dCGPANN's learning hyper-parameters. Table \ref{tab:org31f4c1d}, \ref{tab:orgdbd7826}, \ref{tab:org30a5dcc} and \ref{tab:org37f3db0} tabulate the classifiers' hyper-parameters used for the first phase of our approach. Table \ref{tab:org038dc62} lists the  the dCGPANN hyper-parameters used for the ensemble optimization in the second phase of the proposed method.
The first stage of the training, multiple classifiers with different configurations were trained on the IDC dataset. Amongst the trained classifiers, the best four performing models were chosing to construct the ensemble. The first model was a Resnet50 pretrained on ImagNet which was trained using Transfer Learning \cite{He2016a,Dengb,Long2015}. We froze all layers except the last 69 layer which included the last new fully connected layer that was adopted to the new binary labels. The learning rate (LR) was high due to the fact that most of the domain-specific layers were frozen. The second model was also a Resnet50 but was trained end-to-end. The third and the fourth models were Nasnetlarge and Densenet201 respectively whose training parameters are shown in Table \ref{tab:org30a5dcc} and Table \ref{tab:org37f3db0} \cite{Zoph2018}. 

\begin{table}[htbp]
\caption{\label{tab:org31f4c1d}Hyper-parameters for Classifier 1}
\centering
\begin{tabular}{ll}
hyper-parameter & value\\
\hline
Name & Resnet50 \cite{He2016a}\\
Training & Transfer Learning using ImagNet weights \cite{Dengb}\\
Total params & 23,595,908\\
trainable residual blocks & 10\\
trainable conventional blocks & 20\\
trainable layers & 69\\
trainable params & 18,605,572\\
Non-trainable params & 4,990,336\\
LR & 0.0001\\
LR scheduler & Cyclic LR  \cite{Smith2017c}\\
Optimizer & AdaMax    \cite{Kingma2014}\\
epochs & 200 with earlystopping\\
\end{tabular}
\end{table}
\begin{table}[htbp]
\caption{\label{tab:orgdbd7826}Hyper-parameters for Classifier 2}
\centering
\begin{tabular}{ll}
hyper-parameter & value\\
\hline
Name & ResNet50\\
Training & end-to-end\\
Total params & 23,595,908\\
trainable layers & All\\
trainable params & 23,595,908\\
LR & 0.0001\\
LR scheduler & Cyclic LR\\
Optimizer & SGD\\
epochs & 400 with earlystopping\\
\end{tabular}
\end{table}
\begin{table}[htbp]
\caption{\label{tab:org30a5dcc}Hyper-parameters for Classifier 3}
\centering
\begin{tabular}{ll}
hyper-parameter & value\\
\hline
Name & NasNetLarge     \cite{Zoph2018}\\
Pre-trained & Transfer Learning using ImagNet weights\\
Total params & 84,932,950\\
trainable layers & 662\\
trainable params & 68,365,924\\
Non-trainable params & 16,567,026\\
LR & 1e-5\\
Optimizer & AdaMax\\
epochs & 200 with earlystopping\\
\end{tabular}
\end{table}
\begin{table}[htbp]
\caption{\label{tab:org37f3db0}Hyper-parameters for Classifier 4}
\centering
\begin{tabular}{ll}
hyper-parameter & value\\
\hline
Name & DenseNet201 \cite{Zhu2018}\\
Training & Transfer Learning using ImagNet weights\\
Total params & 18,329,668\\
trainable layers & 570\\
trainable params & 4,705,668\\
Non-trainable params & 13,624,000\\
LR & 0.0001\\
Optimizer & AdaMax\\
\end{tabular}
\end{table}
\begin{table}[htbp]
\caption{\label{tab:org038dc62}dCGP parameters}
\centering
\begin{tabular}{lr}
Paramter & value\\
\hline
r number of rows & 20\\
c number of columns & 5\\
m input features & 8\\
possible kernels & sig, ReLu, tanh, ELU, ISRU\\
l level-back allowed connections & 1\\
population & 7\\
\(\mu\)\textsubscript{a} & 0.05\\
\(\mu\)\textsubscript{c} & 0.05\\
number of iterations & 100\\
number of epochs & 15\\
\end{tabular}
\end{table}

\FloatBarrier 
 The possible kernels used for the dCGPANN are the Sig, ReLu, tanh, ELU and ISRU functions
 which are defined by equations \ref{eq:orgfc4e0f3}, \ref{eq:orgcd69b23}, \ref{eq:orga1ec476}, \ref{eq:org5a14060}
 and \ref{eq:orgb8b7b66} respectively \cite{Nwankpa2018,Carlile2017}. 

\begin{equation}
\label{eq:orgfc4e0f3}
Sig(x) = \frac{1}{1+\exp(-x)}
\end{equation}
\begin{equation}
\label{eq:orgcd69b23}
ReLu(x) = max(0,x) =  
      \begin{cases} 
      x_i & if\  x_i\geq 0 \\
      0 & if\ x_i < 0
   \end{cases}
\end{equation} 
\begin{equation}
\label{eq:orga1ec476}
tanh(x) = \frac{\exp(x)-\exp(-x)}{\exp(x)+\exp(-x)}
\end{equation}

\begin{equation}
\label{eq:org5a14060}
ELU(x) = 
      \begin{cases} 
      x & if\  x > 0 \\
      \alpha \exp(x) -1 & if\ x_i \leq 0
   \end{cases}
\end{equation}

\begin{equation}
\label{eq:orgb8b7b66}
ISRU(x) = x \frac{1}{\sqrt(1 + \alpha x^2)}
\end{equation}

\section{Experimental Setup}
\label{sec:orgf68e06a}
\label{org9f20484}
\subsection{Dataset and The Experimental Setup}
\label{sec:org373eea8}
\qquad The dataset contains patches extracted from the whole slides of
Two hundred seventy-nine patients were diagnosed with IDC. The total
number of the extracted non-overlapping of 50x50 pixels patches is
277524, which are labeled into IDC and non-IDC regions. The number of
patches and the percentage of IDC annotations per patient vary
significantly; thus, the labels of the images are not equal. Fig
\ref{fig:org9f0e102} shows the patches and IDC annotation percentage
histogram. Some visual features distinguish IDC from non-IDC patches,
such as tissue coloration as shown in Figure \ref{fig:orgd935baf} and figure
\ref{fig:org513368a}, which shows negative and positive IDC, respectively. Figure
\ref{fig:orged35253} shows the color maps of annotated whole slides where the yellow regions indicate the presence of IDC, and figure
\ref{fig:org31c7a41} shows a whole slide that was reconstructed using the
coordinates information provided by the dataset. The darker mask
points to the IDC regions on the whole slide image (WSI) \cite{kagglebhi} .
Data visualization of the training image set is shown in figures \ref{fig:org9f0e102}, \ref{fig:orgd935baf}, \ref{fig:org513368a}, \ref{fig:orged35253} and \ref{fig:org31c7a41} \cite{kagglebhi}. 

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/alkhaldieid/Dropbox/third/datasethists.png}
\caption{\label{fig:org9f0e102}Dataset Statistics \cite{kagglebhi}}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/alkhaldieid/Dropbox/third/idcnp.png}
\caption{\label{fig:orgd935baf}IDC negative samples  \cite{kagglebhi}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/alkhaldieid/Dropbox/third/idcpp.png}
\caption{\label{fig:org513368a}IDC positive samples \cite{kagglebhi}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/alkhaldieid/Dropbox/third/idccmap.png}
\caption{\label{fig:orged35253}IDC Colormap in a WSI \cite{kagglebhi}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/alkhaldieid/Dropbox/third/annotatedWSI.png}
\caption{\label{fig:org31c7a41}Reconstructed Annotated WSI \cite{kagglebhi}}
\end{figure}



The dataset was divided into three sets. 70\% of the patients' slides were used as a training dataset, and the remaining patients' slides were
divided into a validation dataset and a test dataset with 15\%
of the slides each. Our model and the base models to which our method
was compared to were implemented using PyTorch, dcgpy and scikit-learn
\cite{Paszke2019,Izzo2020,pedregosa2011scikit}.
Our method was compared to two voting strategies which are the maximum and the weighted average.
\subsection{Evaluation Metrics}
\label{sec:org5c48b86}
The evaluation metrics used for our assessment are the
F1-score, the Balanced Accuracy and the overall accuracy as defined in equations \ref{eq:org596ce0c}, \ref{eq:orgd34b6f4} and \ref{eq:org1886969} respectively \cite{Duchesnay2019,Javeed2019}.
\begin{equation}
\label{eq:org596ce0c}
F_1 = \frac{2}{\frac{1}{Recall} + \frac{1}{Precision}} = \frac{2 Precision \times Recall}{Precision+Recall}
\end{equation}
Where \(Recall\) and \(Precesion\) are defined in \ref{eq:org18152b3} and \ref{eq:orge8719af} \cite{Duchesnay2019}.
\begin{equation}
\label{eq:org18152b3}
Recall = Sensitivity = \frac{TP}{TP+ FP}
\end{equation}
Note that \(TP\) is number of correctly positively classified samples and
\(FP\) is the incorrectly positively classified ones.

\begin{equation}
\label{eq:orge8719af}
Precision = \frac{TP}{TP+ FN}
\end{equation}

\begin{equation}
\label{eq:org1886969}
BAC = \frac{Sensitivity + Specificity}{2}
\end{equation}
where Specivity is defined by equation \ref{eq:orgb251fcc}.

\begin{equation}
\label{eq:orgb251fcc}
Specivity = \frac{TN}{TN+FP}
\end{equation}

\begin{equation}
\label{eq:orgd34b6f4}
Accuracy = \frac{TP+TN}{TP+TN+FP+FN}
\end{equation}

\begin{itemize}
\item Macro-average: is the sum of each class' precision divided by the number of classes.
\item Micro-average: is the sum of the true positives of all classes divided by the number of samples.
\end{itemize}
\section{Results}
\label{sec:org9066dc6}
\label{org2d1e51d}

\qquad This section shows the results of our experiments. The performance of phase 1 trained classifiers as well as the base models on the validation set are shown in table \ref{tab:org068a64f}.
\begin{table}[!hbt]
\caption{\label{tab:org068a64f}Confusion matrices of various models on the validation set}
\centering
\begin{tabular}{lrr}
\hline
classifier 1 & Predicted IDC & Predicted Non-IDC\\
\hline
actual IDC & 0.904239 & 0.095761\\
actual Non-IDC & 0.240137 & 0.759863\\
\hline
classifier 2 & Predicted IDC & Predicted Non-IDC\\
actual IDC & 0.920932 & 0.079068\\
actual Non-IDC & 0.257888 & 0.742112\\
\hline
classifier 3 & Predicted IDC & Predicted Non-IDC\\
actual IDC & 0.931916 & 0.068084\\
actual Non-IDC & 0.320158 & 0.679842\\
\hline
classifier 4 & Predicted IDC & Predicted Non-IDC\\
actual IDC & 0.936776 & 0.063224\\
actual Non-IDC & 0.324413 & 0.675587\\
\hline
Average Voting & Predicted IDC & Predicted Non-IDC\\
actual IDC & 0.936776 & 0.063224\\
actual Non-IDC & 0.324413 & 0.675587\\
\hline
Maximum Voting & Predicted IDC & Predicted Non-IDC\\
actual IDC & 0.931916 & 0.068084\\
actual Non-IDC & 0.320158 & 0.679842\\
\hline
dCGPANN & Predicted IDC & Predicted Non-IDC\\
actual IDC & 0.961916 & 0.038084\\
actual Non-IDC & 0.120158 & 0.879842\\
\hline
\end{tabular}
\end{table}


Our method was compared to the weighted voting, the Maximum voting
and the random forest tree-based ensemble and the outcomes on the
held-out test dataset are reported in Table \ref{tab:orgbc05075} , \ref{tab:org2124f43} and \ref{tab:orgd27e297}.
The results demonstrate a clear advantage of the dCGPANN ensemble over all other voting strategies, as shown in table \ref{tab:orgd27e297}.


\begin{table}[!hbt]
\caption{\label{tab:orgbc05075}Performance of the weighted voting on the test dataset}
\centering
\begin{tabular}{lrrr}
 & precision & recall & f1-score\\
\hline
actual no cancer & 0.84 & 0.93 & 0.89\\
actual cancer & 0.84 & 0.68 & 0.75\\
macro avg & 0.84 & 0.81 & 0.82\\
weighted avg & 0.84 & 0.84 & 0.84\\
\end{tabular}
\end{table}



\begin{table}[!hbt]
\caption{\label{tab:org2124f43}Performance of the Maximum voting on the test dataset}
\centering
\begin{tabular}{lrrr}
 & precision & recall & f1-score\\
\hline
actual no cancer & 0.84 & 0.93 & 0.89\\
actual cancer & 0.84 & 0.68 & 0.75\\
macro avg & 0.84 & 0.81 & 0.82\\
weighted avg & 0.84 & 0.84 & 0.84\\
\end{tabular}
\end{table}


\begin{table}[!hbt]
\caption{\label{tab:orgd27e297}Performance of dCGPANN voting on the test dataset}
\centering
\begin{tabular}{lrrr}
 & precision & recall & f1-score\\
\hline
actual no cancer & 0.96 & 0.94 & 0.95\\
actual cancer & 0.93 & 0.88 & 0.93\\
macro avg & 0.95 & 0.91 & 0.92\\
weighted avg & 0.95 & 0.96 & 0.95\\
\end{tabular}
\end{table}

\begin{table}[!hbt]
\caption{\label{tab:org501fedf}F1 scores and Balanced Accuracy (BAC) comparision of the proposed method with other methods}
\centering
\begin{tabular}{lll}
 & F1-score & Balanced accuracy\\
\hline
[Cruz-Roa 2014]: & 71.80\%, & 84.23\%\\
weighted average & 84\% & 80.588\%\\
Maximum voting & 81\% & 79.487\%\\
Proposed Method & 96\% & 87.618\%\\
\end{tabular}
\end{table}
\FloatBarrier 

\section{Conclusion}
\label{sec:orgea33634}
\label{org30f40b8}

\qquad The accuracy of the Deep Learning models is a cornerstone to the
CAD systems.  It is evident that the advancements of Deep Learning and
GPUs enabled computers to perform better or at least equal to human
experts.  Thus, computerized cancer detection can at the very least
increase the speed of diagnosis.  Our proposed method addresses the
scarcity of labeled data by implementing a bio-inspired algorithm to
construct a more reliable ensemble. The ensemble learns the correct
class based on the misclassifications of the pre-trained models.  The
differential Cartesian Genetic Programming was used to acquire the
most optimum ensemble rule to compensate for the insufficient quantity
of images available for multi-stage training, which is required to
infer accurate mapping between intricate features and correct
classes. Due to its cabability to assign weights and biases to ANNs
and to use SGD for learning them, the dCGPANN proves to be a powerful
tool to optimize the ensemble topology as well as its hyperparameters.
The experimental results exceeded previous hyperparameters search
methods.  Our future research will pay more emphasis on the
optimization of the hyperparameters of the ensemble network.
\bibliographystyle{ieeetr}
\bibliography{../../../work/res/cited_lib}
\end{document}
